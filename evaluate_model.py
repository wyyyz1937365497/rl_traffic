"""
å¼‚æ­¥è¯„ä¼°è„šæœ¬ - æ¯”èµ›çº§åˆ«çš„ä»¿çœŸè¯„ä¼°

ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œè¾“å‡ºOCRç›¸å…³æŒ‡æ ‡
å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»è®­ç»ƒè¿›ç¨‹
"""

import os
import sys
import torch
import argparse
import logging
import traceback as tb
from datetime import datetime
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from .sumo.main import SUMOCompetitionFramework


def setup_evaluation_logger(eval_dir):
    """é…ç½®è¯„ä¼°æ—¥å¿—"""
    log_file = os.path.join(eval_dir, f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='[%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

    return logging.getLogger('evaluation')


def run_evaluation(model_path, sumo_cfg, iteration, eval_dir, device='cuda'):
    """
    è¿è¡Œæ¯”èµ›çº§åˆ«è¯„ä¼°

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        sumo_cfg: SUMOé…ç½®æ–‡ä»¶è·¯å¾„
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        eval_dir: è¯„ä¼°ç»“æœä¿å­˜ç›®å½•
        device: è®¾å¤‡ ('cuda' or 'cpu')
    """
    logger = logging.getLogger('evaluation')

    logger.info("=" * 70)
    logger.info(f"å¼€å§‹è¯„ä¼° - è¿­ä»£ {iteration}")
    logger.info("=" * 70)

    try:
        # åˆ›å»ºæ¡†æ¶å®ä¾‹
        framework = SUMOCompetitionFramework(
            sumo_cfg_path=sumo_cfg,
            model_path=model_path
        )

        # åˆå§‹åŒ–
        framework.parse_config()
        framework.parse_routes()
        framework.initialize_environment()
        framework.load_rl_model()

        # è¿è¡Œä»¿çœŸ
        logger.info("\n[ç¬¬äºŒéƒ¨åˆ†] å¼€å§‹ä»¿çœŸ...")
        framework.run_simulation()

        # è®¡ç®—OCRæŒ‡æ ‡
        logger.info("\n[ç¬¬ä¸‰éƒ¨åˆ†] è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        ocr_metrics = framework.calculate_ocr_metrics()

        # ä¿å­˜ç»“æœ
        result_file = os.path.join(eval_dir, f"eval_iter_{iteration:04d}.json")

        result = {
            'iteration': iteration,
            'timestamp': datetime.now().isoformat(),
            'model_path': model_path,
            'metrics': ocr_metrics,
            'statistics': {
                'total_departed': framework.cumulative_departed,
                'total_arrived': framework.cumulative_arrived,
                'completion_rate': framework.cumulative_arrived / max(framework.cumulative_departed, 1)
            }
        }

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        logger.info("\n" + "=" * 70)
        logger.info(f"è¯„ä¼°å®Œæˆ - è¿­ä»£ {iteration}")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š OCRæŒ‡æ ‡:")
        logger.info(f"  - å…¨å±€OCR: {ocr_metrics.get('global_ocr', 0):.4f}")
        logger.info(f"  - ä¸»è·¯OCR: {ocr_metrics.get('main_road_ocr', 0):.4f}")
        logger.info(f"  - åŒé“OCR: {ocr_metrics.get('ramp_road_ocr', 0):.4f}")
        logger.info(f"  - è½¬å‡ºOCR: {ocr_metrics.get('diverge_road_ocr', 0):.4f}")
        logger.info(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  - æ€»å‡ºå‘è½¦è¾†: {framework.cumulative_departed}")
        logger.info(f"  - æ€»åˆ°è¾¾è½¦è¾†: {framework.cumulative_arrived}")
        logger.info(f"  - å®Œæˆç‡: {result['statistics']['completion_rate']:.2%}")
        logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
        logger.info("=" * 70)

        # å…³é—­SUMO
        framework.close()

        return result

    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}\n{tb.format_exc()}")
        return None


def main():
    parser = argparse.ArgumentParser(description='å¼‚æ­¥æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model-path', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sumo-cfg', type=str, default='sumo/sumo.sumocfg', help='SUMOé…ç½®æ–‡ä»¶')
    parser.add_argument('--iteration', type=int, required=True, help='å½“å‰è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--eval-dir', type=str, default='evaluations', help='è¯„ä¼°ç»“æœç›®å½•')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡ (cuda/cpu)')

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä¼°ç›®å½•
    os.makedirs(args.eval_dir, exist_ok=True)

    # é…ç½®æ—¥å¿—
    logger = setup_evaluation_logger(args.eval_dir)

    # è¿è¡Œè¯„ä¼°
    result = run_evaluation(
        model_path=args.model_path,
        sumo_cfg=args.sumo_cfg,
        iteration=args.iteration,
        eval_dir=args.eval_dir,
        device=args.device
    )

    if result is None:
        sys.exit(1)


if __name__ == "__main__":
    main()
