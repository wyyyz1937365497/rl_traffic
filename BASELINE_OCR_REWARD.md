# åŸºäºBaseline OCRæ¯”è¾ƒçš„å¥–åŠ±å‡½æ•°

## æ ¸å¿ƒæ€æƒ³

é€šè¿‡**åœ¨ç›¸åŒæ­¥æ•°æ¯”è¾ƒOCR**ï¼Œæ¶ˆé™¤OCRè‡ªç„¶ä¸Šå‡çš„å½±å“ï¼š

```
å¥–åŠ± = (å½“å‰OCR - BaselineOCRåœ¨ç›¸åŒæ­¥æ•°) Ã— æƒé‡
```

### ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ

**é—®é¢˜**ï¼šOCRä¼šè‡ªç„¶ä¸Šå‡
- è½¦è¾†åˆ°è¾¾æ•°å¢åŠ 
- è¡Œé©¶è·ç¦»å¢åŠ 
- å³ä½¿ç­–ç•¥ä¸å˜ï¼ŒOCRä¹Ÿä¼šä¸Šå‡

**è§£å†³æ–¹æ¡ˆ**ï¼šç›¸å¯¹æ¯”è¾ƒ
- è®°å½•baselineï¼ˆä¸“å®¶/å‰æ¬¡è®­ç»ƒï¼‰åœ¨æ¯ä¸ªæ­¥æ•°çš„OCR
- å½“å‰è®­ç»ƒæ—¶ï¼Œåœ¨**ç›¸åŒæ­¥æ•°**æ¯”è¾ƒ
- ç›´æ¥åé¦ˆç›¸å¯¹äºbaselineçš„æ”¹è¿›

---

## ä½¿ç”¨æµç¨‹

### æ­¥éª¤1ï¼šç”ŸæˆBaseline OCRæ•°æ®

è¿è¡Œä¸“å®¶ç­–ç•¥ï¼ˆæˆ–ä»»ä½•baselineç­–ç•¥ï¼‰ï¼Œè®°å½•OCRæ›²çº¿ï¼š

```bash
python baseline_ocr_rewards.py \
    --sumo-cfg sumo/sumo.sumocfg \
    --output baseline_ocr/expert_baseline.pkl \
    --max-steps 3600
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
âœ“ SUMOå·²å¯åŠ¨
å¼€å§‹æ”¶é›†baseline OCRæ•°æ®...
  æ­¥éª¤ 0: OCR = 0.0500
  æ­¥éª¤ 500: OCR = 0.4500
  æ­¥éª¤ 1000: OCR = 0.7200
  æ­¥éª¤ 1500: OCR = 0.8500
  æ­¥éª¤ 2000: OCR = 0.9000
  æ­¥éª¤ 2500: OCR = 0.9250
  æ­¥éª¤ 3000: OCR = 0.9400
  æ­¥éª¤ 3500: OCR = 0.9500

âœ“ Baselineç”Ÿæˆå®Œæˆï¼
  æœ€ç»ˆOCR: 0.9500
  è¾“å‡ºæ–‡ä»¶: baseline_ocr/expert_baseline.pkl
```

**è¯´æ˜**ï¼š
- æ¯100æ­¥è®°å½•ä¸€æ¬¡OCR
- ä½¿ç”¨ä¸“å®¶ç­–ç•¥ï¼ˆvTypeä¼˜åŒ– + ä¸»åŠ¨é€Ÿåº¦å¼•å¯¼ï¼‰
- çº¦5-10åˆ†é’Ÿå®Œæˆ

---

### æ­¥éª¤2ï¼šå¼€å§‹è®­ç»ƒï¼ˆè‡ªåŠ¨ä½¿ç”¨Baselineæ¯”è¾ƒï¼‰

è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨åŠ è½½baselineå¹¶è®¡ç®—å¢é‡å¥–åŠ±ï¼š

```bash
python rl_train.py \
    --sumo-cfg sumo/sumo.sumocfg \
    --total-timesteps 1000000 \
    --pretrained bc_checkpoints/best_model.pt
```

**æ—¥å¿—è¾“å‡ºç¤ºä¾‹**ï¼š
```
è·¯å£ J5 å¥–åŠ±: 2.3456
    [OCR delta=+0.0234 (current=0.9234, baseline=0.9000),
     ocr_reward=2.34, speed=0.120, throughput=1.000]
```

---

## å¥–åŠ±å‡½æ•°è¯¦è§£

### OCRå¢é‡å¥–åŠ±ï¼ˆæ ¸å¿ƒï¼‰

```python
baseline_ocr = get_baseline_ocr(current_step)  # ä»æ–‡ä»¶åŠ è½½
current_ocr = compute_current_ocr()
ocr_delta = current_ocr - baseline_ocr
ocr_reward = ocr_delta * 100.0  # æƒé‡100
```

**ç¤ºä¾‹**ï¼š
| æ­¥æ•° | Baseline OCR | å½“å‰OCR | å¢é‡ | OCRå¥–åŠ± |
|------|--------------|---------|------|---------|
| 500  | 0.4500       | 0.4520  | +0.0020 | +0.20 |
| 1000 | 0.7200       | 0.7250  | +0.0050 | +0.50 |
| 1500 | 0.8500       | 0.8400  | -0.0100 | **-1.00** |
| 2000 | 0.9000       | 0.9100  | +0.0100 | +1.00 |

### ç¬æ—¶è¾…åŠ©å¥–åŠ±ï¼ˆè¾ƒå°æƒé‡ï¼‰

```python
# é€Ÿåº¦å¥–åŠ±ï¼ˆé™ä½æƒé‡ï¼‰
speed_reward = (main_speed / 15.0) * 0.05

# æµé‡å¥–åŠ±ï¼ˆä¿æŒï¼‰
throughput_reward = departed_vehicles * 1.0

# æ’é˜Ÿæƒ©ç½š
queue_penalty = -(queue_length) * 0.02

# ç­‰å¾…æƒ©ç½š
waiting_penalty = -max(0, waiting_time - 30) * 0.005

# å†²çªæƒ©ç½š
conflict_penalty = -conflict_risk * 0.05

# ç”Ÿå­˜å¥–åŠ±ï¼ˆå®Œå…¨ç§»é™¤ï¼ï¼‰
survival_reward = 0.0
```

### æ€»å¥–åŠ±

```python
total_reward = (
    ocr_reward +        # OCRå¢é‡ï¼ˆä¸»ï¼Œæƒé‡100ï¼‰
    speed_reward +      # é€Ÿåº¦ï¼ˆè¾…åŠ©ï¼Œæƒé‡0.05ï¼‰
    throughput_reward + # æµé‡ï¼ˆè¾…åŠ©ï¼Œæƒé‡1.0ï¼‰
    queue_penalty +     # æ’é˜Ÿï¼ˆæƒ©ç½šï¼Œæƒé‡0.02ï¼‰
    waiting_penalty +   # ç­‰å¾…ï¼ˆæƒ©ç½šï¼Œæƒé‡0.005ï¼‰
    conflict_penalty    # å†²çªï¼ˆæƒ©ç½šï¼Œæƒé‡0.05ï¼‰
)

# è£å‰ªåˆ° [-10, 10]
total_reward = clip(total_reward, -10, 10)
```

---

## å¥–åŠ±æƒé‡å¯¹æ¯”

| å¥–åŠ±åˆ†é‡ | æƒé‡ | è¯´æ˜ |
|---------|------|------|
| **OCRå¢é‡** | **100.0** | **æ ¸å¿ƒä¿¡å·ï¼ç›´æ¥åé¦ˆæ”¹è¿›** |
| é€Ÿåº¦ | 0.05 | é™ä½ï¼ˆä»0.2â†’0.05ï¼‰ |
| æµé‡ | 1.0 | ä¿æŒ |
| æ’é˜Ÿ | 0.02 | ä¿æŒ |
| ç­‰å¾… | 0.005 | ä¿æŒ |
| å†²çª | 0.05 | ä¿æŒ |
| **ç”Ÿå­˜** | **0.0** | **å®Œå…¨ç§»é™¤ï¼** |

---

## å·¥ä½œåŸç†

### Baseline OCRæ•°æ®ç»“æ„

```python
# baseline_ocr/expert_baseline.pkl
{
    'ocr_history': {
        0: 0.0500,
        100: 0.1500,
        200: 0.2500,
        ...
        3600: 0.9500
    },
    'num_records': 37,
    'interval': 100
}
```

### çº¿æ€§æ’å€¼

å¦‚æœå½“å‰æ­¥æ•°æ²¡æœ‰ç²¾ç¡®è®°å½•ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼ï¼š

```python
# ä¾‹å¦‚ï¼šéœ€è¦æ­¥éª¤ 1234 çš„baseline
# æ‰¾åˆ°æœ€è¿‘çš„è®°å½•ï¼š1200 å’Œ 1300
baseline_1200 = 0.88
baseline_1300 = 0.90

# çº¿æ€§æ’å€¼
ratio = (1234 - 1200) / (1300 - 1200) = 0.34
baseline_1234 = 0.88 + 0.34 * (0.90 - 0.88) = 0.8868
```

---

## é¢„æœŸè®­ç»ƒæ•ˆæœ

### å¥–åŠ±ä¿¡å·

**å¥½çš„è®­ç»ƒ**ï¼ˆæ¨¡å‹æŒç»­æ”¹è¿›ï¼‰ï¼š
```
æ­¥éª¤ 500:  OCR delta = +0.002, reward = +0.2
æ­¥éª¤ 1000: OCR delta = +0.005, reward = +0.5
æ­¥éª¤ 1500: OCR delta = +0.008, reward = +0.8
æ­¥éª¤ 2000: OCR delta = +0.012, reward = +1.2
```

**å·®çš„è®­ç»ƒ**ï¼ˆæ¨¡å‹é€€åŒ–ï¼‰ï¼š
```
æ­¥éª¤ 500:  OCR delta = -0.001, reward = -0.1
æ­¥éª¤ 1000: OCR delta = -0.003, reward = -0.3
æ­¥éª¤ 1500: OCR delta = -0.005, reward = -0.5
```

### è®­ç»ƒæ›²çº¿

ç†æƒ³æƒ…å†µä¸‹ï¼š
- **åˆæœŸ**ï¼šOCRå¢é‡æ¥è¿‘0ï¼ˆä»ä¸“å®¶åˆå§‹åŒ–ï¼‰
- **ä¸­æœŸ**ï¼šOCRå¢é‡é€æ¸ä¸ºæ­£ï¼ˆæ¨¡å‹å¼€å§‹æ”¹è¿›ï¼‰
- **åæœŸ**ï¼šOCRå¢é‡ç¨³å®šåœ¨æ­£å€¼ï¼ˆæŒç»­æ”¹è¿›ï¼‰

---

## é«˜çº§ç”¨æ³•

### 1. ä½¿ç”¨ä¸åŒçš„Baseline

å¯ä»¥æ¯”è¾ƒä¸åŒbaselineï¼š

```python
# ä¸“å®¶ç­–ç•¥baseline
calc = BaselineOCRRewardCalculator(
    baseline_file='baseline_ocr/expert_baseline.pkl'
)

# ä¹‹å‰è®­ç»ƒçš„æ¨¡å‹baseline
calc = BaselineOCRRewardCalculator(
    baseline_file='baseline_ocr/v1_baseline.pkl'
)

# å›ºå®šå€¼baselineï¼ˆfallbackï¼‰
calc = BaselineOCRRewardCalculator(
    baseline_file=None  # ä½¿ç”¨å›ºå®šå€¼0.95
)
```

### 2. è°ƒæ•´OCRå¥–åŠ±æƒé‡

ç¼–è¾‘ `rl_train.py` ç¬¬490è¡Œï¼š

```python
self.reward_calculator = BaselineOCRRewardCalculator(
    baseline_file=baseline_file,
    reward_weight=200.0  # å¢åŠ åˆ°200ï¼ˆé»˜è®¤100ï¼‰
)
```

### 3. ä»å·²æœ‰æ¨¡å‹ç”ŸæˆBaseline

å¦‚æœå·²ç»æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒç”Ÿæˆbaselineï¼š

```python
# ä¿®æ”¹ baseline_ocr_rewards.py
# å°† ExpertPolicy æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹
from your_model import YourModel

model = YourModel()
model.load('checkpoints/your_model.pt')

# åœ¨ä»¿çœŸå¾ªç¯ä¸­ä½¿ç”¨æ¨¡å‹æ§åˆ¶
actions = model.get_action(state)
apply_actions(actions)
```

---

## ç›‘æ§è®­ç»ƒ

### å…³é”®æŒ‡æ ‡

è®­ç»ƒæ—¶å…³æ³¨ï¼š

1. **OCR Delta** - å½“å‰OCRç›¸å¯¹äºbaselineçš„æ”¹è¿›
   - ç›®æ ‡ï¼šæŒç»­ä¸ºæ­£
   - è­¦å‘Šï¼šæŒç»­ä¸ºè´Ÿï¼ˆæ¨¡å‹åœ¨é€€åŒ–ï¼‰

2. **OCR Reward** - OCRå¢é‡å¥–åŠ±
   - ç›®æ ‡ï¼šé€æ¸å¢å¤§
   - è­¦å‘Šï¼šéœ‡è¡æˆ–ä¸‹é™

3. **Total Reward** - æ€»å¥–åŠ±
   - ç›®æ ‡ï¼šç¨³å®šä¸Šå‡
   - è­¦å‘Šï¼šæŒç»­ä¸ºè´Ÿ

### TensorBoard

```bash
tensorboard --logdir logs
```

æŸ¥çœ‹æŒ‡æ ‡ï¼š
- `train/ocr_delta` - OCRå¢é‡
- `train/ocr_reward` - OCRå¥–åŠ±
- `train/total_reward` - æ€»å¥–åŠ±

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šæœªæ‰¾åˆ°baselineæ–‡ä»¶

**æ—¥å¿—**ï¼š
```
[WARNING] æœªæ‰¾åˆ°baselineæ–‡ä»¶: baseline_ocr/expert_baseline.pkl
[WARNING] å°†ä½¿ç”¨å›ºå®šbaseline OCR = 0.95
```

**è§£å†³**ï¼š
```bash
python baseline_ocr_rewards.py --sumo-cfg sumo/sumo.sumocfg
```

### é—®é¢˜2ï¼šOCRå¢é‡å§‹ç»ˆä¸º0

**å¯èƒ½åŸå› **ï¼š
- å½“å‰æ¨¡å‹ä¸baselineæ€§èƒ½ç›¸åŒ
- è®¡ç®—OCRæ—¶å‡ºé”™

**æ£€æŸ¥**ï¼š
- æŸ¥çœ‹æ—¥å¿—ä¸­çš„ `current_ocr` å’Œ `baseline_ocr`
- ç¡®è®¤OCRè®¡ç®—æ­£ç¡®

### é—®é¢˜3ï¼šOCRå¢é‡æŒç»­ä¸ºè´Ÿ

**å¯èƒ½åŸå› **ï¼š
- æ¨¡å‹æ€§èƒ½æ¯”baselineå·®
- å­¦ä¹ ç‡å¤ªé«˜ï¼Œè®­ç»ƒä¸ç¨³å®š

**è§£å†³**ï¼š
- é™ä½å­¦ä¹ ç‡ï¼š`--lr 1e-4`
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
- å¢åŠ è®­ç»ƒæ—¶é—´

---

## æ€»ç»“

### å…³é”®æ”¹è¿›

1. âœ… **æ¶ˆé™¤OCRè‡ªç„¶ä¸Šå‡åå·®** - é€šè¿‡ç›¸åŒæ­¥æ•°æ¯”è¾ƒ
2. âœ… **ç›´æ¥åé¦ˆæ”¹è¿›** - OCRå¢é‡ = ç›¸å¯¹äºbaselineçš„è¿›æ­¥
3. âœ… **ç›®æ ‡å¯¼å‘æ˜ç¡®** - å¥–åŠ± = (å½“å‰ - baseline) Ã— 100
4. âœ… **ç”Ÿå­˜å¥–åŠ±ç§»é™¤** - å®Œå…¨ç§»é™¤ï¼Œä¸ä¾èµ–ç”Ÿå­˜
5. âœ… **è¾…åŠ©å¥–åŠ±ä¿æŒ** - é€Ÿåº¦ã€æµé‡ç­‰è¾…åŠ©ä¿¡å·

### è®­ç»ƒæµç¨‹

```bash
# æ­¥éª¤0ï¼šç”Ÿæˆbaselineï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
python baseline_ocr_rewards.py --sumo-cfg sumo/sumo.sumocfg

# æ­¥éª¤1ï¼šæ”¶é›†ä¸“å®¶æ•°æ®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
python collect_expert_demos.py --num-episodes 5

# æ­¥éª¤2ï¼šè¡Œä¸ºå…‹éš†ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
python behavior_cloning.py --demo-file expert_demos/expert_demonstrations.pkl

# æ­¥éª¤3ï¼šRLå¾®è°ƒï¼ˆè‡ªåŠ¨ä½¿ç”¨baselineæ¯”è¾ƒï¼‰
python rl_train.py --pretrained bc_checkpoints/best_model.pt
```

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼ğŸ¯

**æ ¸å¿ƒä¼˜åŠ¿**ï¼šæ¨¡å‹èƒ½æ˜ç¡®çŸ¥é“"æˆ‘æ¯”ä¸“å®¶/å‰ä¸€æ¬¡å¥½äº†å¤šå°‘"ï¼Œå¹¶é€šè¿‡å¥–åŠ±ä¿¡å·å¾—åˆ°åé¦ˆã€‚
