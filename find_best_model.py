"""
æŸ¥æ‰¾æœ€ä½³æ¨¡å‹å·¥å…·

ç”¨äºåˆ†ææ‰€æœ‰è¯„ä¼°ç»“æœï¼Œæ‰¾å‡ºå®Œæˆç‡æœ€é«˜çš„æ¨¡å‹
"""

import json
import glob
from pathlib import Path
import sys

def find_best_model(eval_dir='competition_results'):
    """
    æŸ¥æ‰¾æœ€ä½³æ¨¡å‹

    Args:
        eval_dir: è¯„ä¼°ç»“æœç›®å½•

    Returns:
        best_model: æœ€ä½³æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    # æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°ç»“æœJSON
    json_pattern = Path(eval_dir) / "eval_iter_*.json"
    json_files = list(glob.glob(str(json_pattern)))

    if not json_files:
        print(f"é”™è¯¯: åœ¨ {eval_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        print(f"è¯·å…ˆè¿è¡Œ: python run_evaluation.py --checkpoint all")
        return None

    # è¯»å–æ‰€æœ‰ç»“æœ
    results = []
    for json_file in sorted(json_files):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                results.append({
                    'json_file': json_file,
                    'iteration': data.get('iteration', 0),
                    'completion_rate': data['statistics'].get('completion_rate', 0.0),
                    'total_departed': data['statistics'].get('total_departed', 0),
                    'total_arrived': data['statistics'].get('total_arrived', 0),
                    'pickle_file': data.get('pickle_file', ''),
                    'timestamp': data.get('timestamp', '')
                })
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å– {json_file}: {e}")

    if not results:
        print(f"é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return None

    # æŒ‰å®Œæˆç‡æ’åº
    results.sort(key=lambda x: x['completion_rate'], reverse=True)

    return results

def print_results_table(results):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    print("\n" + "=" * 100)
    print(f"{'æ’å':<6} {'è¿­ä»£':<8} {'å®Œæˆç‡':<12} {'å‡ºå‘è½¦è¾†':<12} {'åˆ°è¾¾è½¦è¾†':<12} {'Pickleæ–‡ä»¶':<40}")
    print("=" * 100)

    for i, r in enumerate(results, 1):
        # æå–æ–‡ä»¶å
        pickle_name = Path(r['pickle_file']).name if r['pickle_file'] else 'N/A'

        # æ ‡è®°æœ€ä½³æ¨¡å‹
        marker = "ğŸ† " if i == 1 else "   "

        print(f"{marker}{i:<3} {r['iteration']:<8} {r['completion_rate']:<12.4f} "
              f"{r['total_departed']:<12} {r['total_arrived']:<12} {pickle_name:<40}")

    print("=" * 100)

def print_summary(results):
    """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
    if not results:
        return

    best = results[0]
    worst = results[-1]
    avg_rate = sum(r['completion_rate'] for r in results) / len(results)

    print("\n" + "=" * 100)
    print("ğŸ“Š è¯„ä¼°æ±‡æ€»")
    print("=" * 100)
    print(f"æ€»è¯„ä¼°æ¬¡æ•°: {len(results)}")
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹:")
    print(f"   è¿­ä»£: {best['iteration']}")
    print(f"   å®Œæˆç‡: {best['completion_rate']:.4f} ({best['completion_rate']*100:.2f}%)")
    print(f"   å‡ºå‘è½¦è¾†: {best['total_departed']}")
    print(f"   åˆ°è¾¾è½¦è¾†: {best['total_arrived']}")
    print(f"   Pickleæ–‡ä»¶: {best['pickle_file']}")

    print(f"\nâš ï¸  æœ€å·®æ¨¡å‹:")
    print(f"   è¿­ä»£: {worst['iteration']}")
    print(f"   å®Œæˆç‡: {worst['completion_rate']:.4f} ({worst['completion_rate']*100:.2f}%)")

    print(f"\nğŸ“ˆ å¹³å‡å®Œæˆç‡: {avg_rate:.4f} ({avg_rate*100:.2f}%)")

    # æ”¹è¿›ç©ºé—´
    improvement = (best['completion_rate'] - worst['completion_rate']) * 100
    print(f"\nğŸ“Š æ”¹è¿›å¹…åº¦: {improvement:.2f}%")

    print("=" * 100)

def check_pickle_exists(results):
    """æ£€æŸ¥pklæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("\n" + "=" * 100)
    print("ğŸ“ æ–‡ä»¶æ£€æŸ¥")
    print("=" * 100)

    for r in results[:5]:  # åªæ£€æŸ¥å‰5ä¸ª
        pickle_path = Path(r['pickle_file'])
        exists = "âœ“" if pickle_path.exists() else "âœ—"
        size_mb = pickle_path.stat().st_size / (1024*1024) if pickle_path.exists() else 0

        print(f"{exists} è¿­ä»£ {r['iteration']:<4} {pickle_path.name:<50} "
              f"{'{:.2f} MB'.format(size_mb) if size_mb > 0 else 'ä¸å­˜åœ¨'}")

    print("=" * 100)

def main():
    import argparse

    parser = argparse.ArgumentParser(description='æŸ¥æ‰¾æœ€ä½³è®­ç»ƒæ¨¡å‹')
    parser.add_argument('--eval-dir', type=str, default='competition_results',
                       help='è¯„ä¼°ç»“æœç›®å½• (é»˜è®¤: competition_results)')
    parser.add_argument('--top', type=int, default=10,
                       help='æ˜¾ç¤ºå‰Nä¸ªæ¨¡å‹ (é»˜è®¤: 10)')

    args = parser.parse_args()

    results = find_best_model(args.eval_dir)

    if not results:
        sys.exit(1)

    # æ‰“å°æ‰€æœ‰ç»“æœ
    print_results_table(results[:args.top])

    # æ‰“å°æ±‡æ€»
    print_summary(results)

    # æ£€æŸ¥æ–‡ä»¶
    check_pickle_exists(results)

    # æ‰“å°æ¨èå‘½ä»¤
    best = results[0]
    print("\n" + "=" * 100)
    print("ğŸ’¡ æ¨èæäº¤å‘½ä»¤")
    print("=" * 100)
    print(f"\næœ€ä½³æ¨¡å‹æ˜¯è¿­ä»£ {best['iteration']}ï¼Œå®Œæˆç‡ {best['completion_rate']*100:.2f}%")
    print(f"\næäº¤æ–‡ä»¶ä½äº: {best['pickle_file']}")
    print(f"\nå¦‚éœ€é‡æ–°ç”Ÿæˆï¼Œè¿è¡Œ:")
    print(f"  python run_evaluation.py --checkpoint checkpoints/checkpoint_iter_{best['iteration']:04d}.pt")
    print("=" * 100 + "\n")

if __name__ == "__main__":
    main()
