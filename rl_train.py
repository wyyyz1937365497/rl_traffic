"""
å¤šæ™ºèƒ½ä½“è·¯å£äº¤é€šæ§åˆ¶ç³»ç»Ÿ - ç®€åŒ–ç‰ˆ
åªæ”¯æŒCUDAè®­ç»ƒ + æ–‡ä»¶IOå¹¶è¡Œæ•°æ®æ”¶é›†
"""

import os
from vehicle_type_config import normalize_speed, get_vehicle_max_speed
import sys
import argparse
import json
import time
import shutil
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
import pickle
from multiprocessing import Process
import multiprocessing
import subprocess
import threading
import traceback as tb

from junction_agent import JUNCTION_CONFIGS

from junction_network import create_junction_model, NetworkConfig
from junction_trainer import PPOConfig, MultiAgentPPOTrainer

# å°è¯•å¯¼å…¥libsumo
try:
    import libsumo as traci_wrapper
    USE_LIBSUMO = True
except ImportError:
    import traci as traci_wrapper
    USE_LIBSUMO = False


def print_header(title: str):
    print("=" * 70)
    print(title)
    print("=" * 70)


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("\nç¯å¢ƒæ£€æŸ¥:")

    try:
        import libsumo
        print("  âœ“ libsumo å¯ç”¨ï¼ˆé«˜é€Ÿæ¨¡å¼ï¼‰")
    except ImportError:
        print("  âš  libsumo ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ traci")

    cuda_available = torch.cuda.is_available()
    print(f"  âœ“ CUDA: {cuda_available}")
    if cuda_available:
        print(f"    GPU: {torch.cuda.get_device_name(0)}")
        print(f"    GPUæ•°é‡: {torch.cuda.device_count()}")

    cpu_count = multiprocessing.cpu_count()
    print(f"  âœ“ CPUæ ¸å¿ƒæ•°: {cpu_count}")

    # æ£€æµ‹WSL
    if os.path.exists('/proc/version'):
        with open('/proc/version', 'r') as f:
            if 'microsoft' in f.read().lower():
                print("  âœ“ WSL ç¯å¢ƒ")

    print(f"\næ¨èé…ç½®: --num-envs {min(4, cpu_count)}")


def start_async_evaluation(model_path, sumo_cfg, iteration, eval_dir='evaluations', device='cuda'):
    """
    å¯åŠ¨å¼‚æ­¥è¯„ä¼°è¿›ç¨‹ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        sumo_cfg: SUMOé…ç½®æ–‡ä»¶
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        eval_dir: è¯„ä¼°ç»“æœç›®å½•
        device: è®¾å¤‡
    """
    def run_in_thread():
        try:
            cmd = [
                sys.executable, 'evaluate_model.py',
                '--model-path', model_path,
                '--sumo-cfg', sumo_cfg,
                '--iteration', str(iteration),
                '--eval-dir', eval_dir,
                '--device', device
            ]

            # å¯åŠ¨è¯„ä¼°è¿›ç¨‹ï¼ˆä¸ç­‰å¾…ï¼‰
            subprocess.run(cmd, check=True, capture_output=True, text=True)
        except Exception as e:
            print(f"  âš ï¸  å¼‚æ­¥è¯„ä¼°å¤±è´¥: {e}")

    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
    thread = threading.Thread(target=run_in_thread, daemon=True)
    thread.start()

    return thread


def create_libsumo_environment(sumo_cfg: str, seed: int = 42):
    """åˆ›å»ºlibsumoç¯å¢ƒ"""
    import logging
    import traceback as tb

    # ä¸ºworkeré…ç½®æ—¥å¿—
    worker_logger = logging.getLogger(f'sumo_worker')
    if not worker_logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))
        worker_logger.addHandler(handler)
        worker_logger.setLevel(logging.INFO)

    import junction_agent  # å¯¼å…¥æ¨¡å—æœ¬èº«ï¼ˆç”¨äºè®¾ç½®traciè¿æ¥ï¼‰
    from junction_agent import JunctionAgent, SubscriptionManager

    class Environment:
        def __init__(self, sumo_cfg: str, seed: int):
            self.sumo_cfg = sumo_cfg
            self.seed = seed
            self.agents = {}
            self.is_running = False
            self.current_step = 0
            self.logger = worker_logger

            # åˆ›å»ºè®¢é˜…ç®¡ç†å™¨ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
            self.sub_manager = SubscriptionManager()

            try:
                for junc_id in JUNCTION_CONFIGS.keys():
                    self.agents[junc_id] = JunctionAgent(
                        JUNCTION_CONFIGS[junc_id],
                        self.sub_manager
                    )
                self.logger.info(f"Environmentåˆå§‹åŒ–å®Œæˆï¼ˆè®¢é˜…æ¨¡å¼ï¼‰ï¼Œç§å­={seed}")
            except Exception as e:
                self.logger.error(f"Environmentåˆå§‹åŒ–å¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def reset(self):
            """é‡ç½®ç¯å¢ƒ"""
            try:
                self._start_sumo()
                self.current_step = 0

                for agent in self.agents.values():
                    agent.state_history.clear()

                # é‡ç½®å¥–åŠ±è®¡ç®—å™¨
                if hasattr(self, 'reward_calculator'):
                    self.reward_calculator.reset()

                # 1. åˆå§‹çƒ­èº«æ­¥è¿›
                for _ in range(10):
                    traci_wrapper.simulationStep()
                    self.current_step += 1

                # 2. è®¾ç½®è®¢é˜…ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
                self._setup_subscriptions()

                # ========== å…³é”®ä¿®å¤ï¼šåˆ·æ–°è®¢é˜…æ•°æ® ==========
                # è®¢é˜…è¯·æ±‚å‘å‡ºåï¼Œå¿…é¡»æ‰§è¡Œä¸€æ¬¡ simulationStep æ‰ä¼šæœ‰æ•°æ®è¿”å›
                traci_wrapper.simulationStep()
                self.current_step += 1

                # ç„¶åå¿…é¡»è°ƒç”¨ update_results å°†æ•°æ®ä» traci æ‹‰å–åˆ° SubscriptionManager ç¼“å­˜ä¸­
                self.sub_manager.update_results()
                # ==========================================

                # 4. è§‚å¯ŸçŠ¶æ€ï¼ˆæ­¤æ—¶ edge_results å·²æœ‰æ•°æ®ï¼‰
                observations = {junc_id: self.agents[junc_id].observe() for junc_id in self.agents.keys()}
                self.logger.info(f"ç¯å¢ƒé‡ç½®å®Œæˆï¼ˆè®¢é˜…æ¨¡å¼ï¼‰ï¼Œcurrent_step={self.current_step}")
                return observations

            except Exception as e:
                self.logger.error(f"ç¯å¢ƒresetå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _setup_subscriptions(self):
            """è®¾ç½®æ‰€æœ‰è·¯å£çš„è®¢é˜…ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰"""
            try:
                for agent in self.agents.values():
                    agent.setup_subscriptions()
                self.logger.info(f"è®¢é˜…è®¾ç½®å®Œæˆï¼Œè¦†ç›– {len(self.agents)} ä¸ªè·¯å£")
            except Exception as e:
                self.logger.error(f"è®¾ç½®è®¢é˜…å¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def step(self, actions):
            """æ‰§è¡Œä¸€æ­¥"""
            import time
            try:
                step_start = time.time()

                # åº”ç”¨åŠ¨ä½œ
                self._apply_actions(actions)

                # ä»¿çœŸä¸€æ­¥
                traci_wrapper.simulationStep()
                self.current_step += 1

                # ========== å…³é”®ä¿®å¤ï¼šæ›´æ–°è®¢é˜…ç»“æœ ==========
                # å¿…é¡»åœ¨ observe() ä¹‹å‰è°ƒç”¨ï¼Œå¦åˆ™è®¢é˜…æ•°æ®ä¸ºç©º
                self.sub_manager.update_results()

                # ä¸ºæ–°è½¦è¾†è®¾ç½®è®¢é˜…
                current_vehicles = set(traci_wrapper.vehicle.getIDList())
                new_vehicles = current_vehicles - self.sub_manager.subscribed_vehicles
                if new_vehicles:
                    self.sub_manager.setup_vehicle_subscription(list(new_vehicles))

                # æ¸…ç†å·²ç¦»å¼€çš„è½¦è¾†
                self.sub_manager.cleanup_left_vehicles(current_vehicles)

                # è§‚å¯Ÿæ–°çŠ¶æ€ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
                obs_start = time.time()
                observations = {junc_id: self.agents[junc_id].observe() for junc_id in self.agents.keys()}
                obs_time = (time.time() - obs_start) * 1000  # ms

                # è®¡ç®—å¥–åŠ±
                rewards = self._compute_rewards()
                done = self.current_step >= 3600

                # æ€§èƒ½ç›‘æ§ï¼ˆæ¯100æ­¥è®°å½•ä¸€æ¬¡ï¼‰
                if self.current_step % 100 == 0:
                    step_time = (time.time() - step_start) * 1000  # ms
                    self.logger.debug(f"Step {self.current_step}: æ€»è€—æ—¶={step_time:.1f}ms, è§‚å¯Ÿ={obs_time:.1f}ms")

                return observations, rewards, done, {}

            except Exception as e:
                self.logger.error(f"ç¯å¢ƒstepå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _start_sumo(self):
            """å¯åŠ¨SUMO"""
            import sys
            import traci as traci_global  # å¯¼å…¥å…¨å±€traciæ¨¡å—

            try:
                if self.is_running:
                    try:
                        traci_wrapper.close()
                        self.logger.debug("å…³é—­æ—§çš„SUMOè¿æ¥")
                    except Exception as e:
                        self.logger.warning(f"å…³é—­SUMOè¿æ¥æ—¶å‡ºé”™: {e}")

                sumo_binary = "sumo"

                if USE_LIBSUMO:
                    sumo_cmd = [sumo_binary, "-c", self.sumo_cfg, "--no-warnings", "true", "--seed", str(self.seed)]
                    traci_wrapper.start(sumo_cmd)
                else:
                    sumo_cmd = [sumo_binary, "-c", self.sumo_cfg, "--remote-port", "0", "--no-warnings", "true", "--seed", str(self.seed)]
                    traci_wrapper.start(sumo_cmd)

                self.is_running = True
                self.logger.info(f"SUMOå·²å¯åŠ¨ (seed={self.seed})")

                # å…³é”®ä¿®å¤ï¼šè®¾ç½®è®¢é˜…æ¨¡å¼æ¨¡å—çš„traciè¿æ¥
                # 1. è®¾ç½®å…¨å±€traciæ¨¡å—ï¼ˆsys.modulesï¼‰
                sys.modules['traci'] = traci_wrapper
                # 2. ç›´æ¥è®¾ç½®è®¢é˜…æ¨¡å¼æ¨¡å—çš„traciå±æ€§ï¼ˆå› ä¸ºæ¨¡å—çº§åˆ«å¼•ç”¨å·²å›ºå®šï¼‰
                junction_agent.traci = traci_wrapper
                self.logger.debug("å·²è®¾ç½®traciè¿æ¥ï¼ˆè®¢é˜…æ¨¡å¼å…¼å®¹ï¼‰")

            except Exception as e:
                self.logger.error(f"å¯åŠ¨SUMOå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _apply_actions(self, actions):
            """åº”ç”¨åŠ¨ä½œåˆ°è½¦è¾†"""
            failed_count = 0

            for junc_id, action_dict in actions.items():
                for veh_id, action in action_dict.items():
                    try:
                        speed_limit = 13.89
                        target_speed = speed_limit * (0.3 + 0.9 * action)
                        traci_wrapper.vehicle.setSpeed(veh_id, target_speed)
                    except Exception as e:
                        failed_count += 1
                        if failed_count <= 3:  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                            self.logger.debug(f"è®¾ç½®è½¦è¾† {veh_id} é€Ÿåº¦å¤±è´¥: {e}")

            if failed_count > 3:
                self.logger.debug(f"æ€»è®¡ {failed_count} ä¸ªè½¦è¾†é€Ÿåº¦è®¾ç½®å¤±è´¥")

        def _compute_rewards(self):
            """è®¡ç®—å¥–åŠ±ï¼ˆä½¿ç”¨æ”¹è¿›ç‰ˆï¼ŒåŒ…å«æ­£å‘å¥–åŠ±ï¼‰"""
            # ä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±è®¡ç®—å™¨
            from improved_rewards import ImprovedRewardCalculator

            if not hasattr(self, 'reward_calculator'):
                self.reward_calculator = ImprovedRewardCalculator()

            # è·å–ç¯å¢ƒç»Ÿè®¡ä¿¡æ¯
            try:
                ocr = self._compute_current_ocr()
            except:
                ocr = 0.0

            env_stats = {
                'ocr': ocr,
                'step': self.current_step
            }

            rewards = self.reward_calculator.compute_rewards(self.agents, env_stats)

            # è°ƒè¯•ï¼šæ¯1000æ­¥æ‰“å°ä¸€æ¬¡å¥–åŠ±è¯¦æƒ…
            if self.current_step % 1000 == 0 and self.current_step > 0:
                for junc_id, reward in rewards.items():
                    agent = self.agents.get(junc_id)
                    if agent and hasattr(agent, 'reward_breakdown') and agent.reward_breakdown:
                        bd = agent.reward_breakdown
                        self.logger.info(
                            f"è·¯å£ {junc_id} å¥–åŠ±: {reward:.4f} "
                            f"[departure:{bd.get('departure_reward', 0):.2f}, "
                            f"flow:{bd.get('flow_reward', 0):.2f}, "
                            f"speed:{bd.get('speed_reward', 0):.2f}, "
                            f"gap:{bd.get('gap_reward', 0):.2f}, "
                            f"no_stops:{bd.get('no_stops_reward', 0):.2f}, "
                            f"capacity:{bd.get('capacity_reward', 0):.2f}, "
                            f"queue:{bd.get('queue_penalty', 0):.2f}, "
                            f"wait:{bd.get('waiting_penalty', 0):.2f}, "
                            f"conflict:{bd.get('conflict_penalty', 0):.2f}, "
                            f"survival:{bd.get('survival_bonus', 0):.2f}]"
                        )
                    else:
                        self.logger.info(f"è·¯å£ {junc_id} å¥–åŠ±: {reward:.4f}")

            return rewards

        def _compute_current_ocr(self) -> float:
            """
            è®¡ç®—å½“å‰OCRï¼ˆç¬¦åˆå®˜æ–¹è¯„æµ‹å…¬å¼ï¼‰

            å®˜æ–¹å…¬å¼:
            OCR = (N_arrived + Î£(d_i_traveled / d_i_total)) / N_total

            å…¶ä¸­:
            - N_arrived: å·²åˆ°è¾¾è½¦è¾†æ•°
            - d_i_traveled: åœ¨é€”è½¦è¾†iå·²è¡Œé©¶çš„è·ç¦»
            - d_i_total: åœ¨é€”è½¦è¾†içš„ODè·¯å¾„æ€»é•¿åº¦
            - N_total: æ€»è½¦è¾†æ•°ï¼ˆå·²åˆ°è¾¾ + åœ¨é€”ï¼‰
            """
            try:
                import traci

                # å·²åˆ°è¾¾è½¦è¾†æ•°
                n_arrived = traci.simulation.getArrivedNumber()

                # åœ¨é€”è½¦è¾†å®Œæˆåº¦
                enroute_completion = 0.0
                for veh_id in traci.vehicle.getIDList():
                    try:
                        # è·å–è½¦è¾†å·²è¡Œé©¶è·ç¦»
                        current_edge = traci.vehicle.getRoadID(veh_id)
                        current_position = traci.vehicle.getLanePosition(veh_id)
                        route_edges = traci.vehicle.getRoute(veh_id)

                        # è®¡ç®—å·²è¡Œé©¶è·ç¦»
                        traveled_distance = 0.0
                        for edge in route_edges:
                            if edge == current_edge:
                                # å½“å‰è¾¹ï¼ŒåŠ ä¸Šå½“å‰ä½ç½®
                                traveled_distance += current_position
                                break
                            else:
                                # å·²é€šè¿‡çš„è¾¹ï¼ŒåŠ ä¸Šè¾¹å…¨é•¿
                                try:
                                    edge_length = traci.edge.getLength(edge)
                                    traveled_distance += edge_length
                                except:
                                    # å¦‚æœè¾¹ä¸å­˜åœ¨ï¼Œå°è¯•è·å–è½¦é“é•¿åº¦
                                    try:
                                        lane_id = f"{edge}_0"
                                        edge_length = traci.lane.getLength(lane_id)
                                        traveled_distance += edge_length
                                    except:
                                        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼100m
                                        traveled_distance += 100.0

                        # è®¡ç®—æ€»è·¯å¾„é•¿åº¦
                        total_distance = 0.0
                        for edge in route_edges:
                            try:
                                edge_length = traci.edge.getLength(edge)
                                total_distance += edge_length
                            except:
                                try:
                                    lane_id = f"{edge}_0"
                                    edge_length = traci.lane.getLength(lane_id)
                                    total_distance += edge_length
                                except:
                                    total_distance += 100.0

                        # è®¡ç®—è¯¥è½¦è¾†çš„å®Œæˆåº¦
                        if total_distance > 0:
                            completion_ratio = min(traveled_distance / total_distance, 1.0)
                            enroute_completion += completion_ratio

                    except Exception as e:
                        # å¦‚æœæŸè¾†è½¦è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                        continue

                # æ€»è½¦è¾†æ•° = å·²åˆ°è¾¾ + åœ¨é€”
                n_total = n_arrived + len(traci.vehicle.getIDList())

                if n_total == 0:
                    return 0.0

                # OCR = (å·²åˆ°è¾¾ + åœ¨é€”è½¦è¾†å®Œæˆåº¦ä¹‹å’Œ) / æ€»è½¦è¾†æ•°
                ocr = (n_arrived + enroute_completion) / n_total
                return min(ocr, 1.0)

            except Exception as e:
                self.logger.debug(f"è®¡ç®—OCRå¤±è´¥: {e}")
                return 0.0

        def close(self):
            """å…³é—­ç¯å¢ƒ"""
            if self.is_running:
                try:
                    traci_wrapper.close()
                    self.logger.info("SUMOè¿æ¥å·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­SUMOè¿æ¥æ—¶å‡ºé”™: {e}")
                self.is_running = False

    return Environment(sumo_cfg, seed)


def worker_process(worker_id, sumo_cfg, output_dir, seed, model_state, use_cuda):
    """å·¥ä½œè¿›ç¨‹ - æ–‡ä»¶IOç‰ˆæœ¬"""
    import traceback
    import logging

    # é…ç½®workeræ—¥å¿—
    worker_logger = logging.getLogger(f'worker_{worker_id}')
    if not worker_logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter(f'[Worker-{worker_id}] [%(levelname)s] %(message)s'))
        worker_logger.addHandler(handler)
        worker_logger.setLevel(logging.INFO)

    try:
        import time
        worker_logger.info(f"Worker {worker_id} å¯åŠ¨ï¼Œseed={seed}")
        worker_start = time.time()

        np.random.seed(seed + worker_id)
        torch.manual_seed(seed + worker_id)

        device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'
        worker_logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # åˆ›å»ºç¯å¢ƒ
        env = create_libsumo_environment(sumo_cfg, seed)
        worker_logger.info("ç¯å¢ƒåˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºæ¨¡å‹
        model = create_junction_model(JUNCTION_CONFIGS)
        model.load_state_dict(model_state)
        model.to(device)
        model.eval()
        worker_logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")

        # æ”¶é›†ç»éªŒ - è¿è¡Œå®Œæ•´çš„3600æ­¥episode
        episode_start = time.time()
        obs = env.reset()
        experiences = []
        total_rewards = {}
        step_count = 0

        # è¿è¡Œå®Œæ•´çš„episodeï¼Œç›´åˆ°ç¯å¢ƒdone
        while True:
            # å‡†å¤‡è§‚å¯Ÿ
            obs_tensors = {}
            vehicle_obs = {}

            for junc_id, agent in env.agents.items():
                try:
                    state_vec = agent.get_state_vector()
                    obs_tensors[junc_id] = torch.tensor(state_vec, dtype=torch.float32, device=device).unsqueeze(0)

                    controlled = agent.get_controlled_vehicles()
                    vehicle_obs[junc_id] = {
                        'main': _get_vehicle_features(controlled['main'], device) if controlled['main'] else None,
                        'ramp': _get_vehicle_features(controlled['ramp'], device) if controlled['ramp'] else None,
                        'diverge': _get_vehicle_features(controlled['diverge'], device) if controlled['diverge'] else None
                    }
                except Exception as e:
                    worker_logger.debug(f"è·¯å£ {junc_id} è§‚å¯Ÿå¤±è´¥: {e}")

            if not obs_tensors:
                worker_logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è§‚å¯Ÿï¼Œè·³è¿‡æ­¤æ­¥")
                break

            # è·å–åŠ¨ä½œ
            try:
                with torch.no_grad():
                    actions, values, info = model(obs_tensors, vehicle_obs, deterministic=False)
            except Exception as e:
                worker_logger.error(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                break

            # è½¬æ¢åŠ¨ä½œ
            action_dict = {}
            for junc_id, action in actions.items():
                action_dict[junc_id] = {}
                try:
                    controlled = env.agents[junc_id].get_controlled_vehicles()

                    if controlled['main'] and 'main' in action:
                        for veh_id in controlled['main'][:1]:
                            action_dict[junc_id][veh_id] = action['main'].item()

                    if controlled['ramp'] and 'ramp' in action:
                        for veh_id in controlled['ramp'][:1]:
                            action_dict[junc_id][veh_id] = action['ramp'].item()
                except Exception as e:
                    worker_logger.debug(f"è·¯å£ {junc_id} åŠ¨ä½œè½¬æ¢å¤±è´¥: {e}")

            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                next_obs, rewards, done, info = env.step(action_dict)
            except Exception as e:
                worker_logger.error(f"ç¯å¢ƒstepå¤±è´¥: {e}\n{traceback.format_exc()}")
                break

            # å­˜å‚¨ç»éªŒï¼ˆç°åœ¨å¯ä»¥è·å–rewardäº†ï¼‰
            # è°ƒè¯•ï¼šç¬¬ä¸€æ¬¡stepæ—¶æ‰“å°rewardså­—å…¸
            if step_count == 0:
                worker_logger.info(f"rewardså­—å…¸é”®: {list(rewards.keys())}")
                worker_logger.info(f"env.agentså­—å…¸é”®: {list(env.agents.keys())}")

            for junc_id in env.agents.keys():
                try:
                    reward = rewards.get(junc_id, 0.0)

                    # è°ƒè¯•ï¼šç¬¬ä¸€æ¬¡stepæ—¶æ‰“å°æ¯ä¸ªè·¯å£çš„å¥–åŠ±
                    if step_count == 0:
                        worker_logger.info(f"è·¯å£ {junc_id} å¥–åŠ±: {reward:.6f}")

                    value = values.get(junc_id, torch.tensor(0.0))
                    log_prob = _compute_log_prob(info.get(junc_id, {}), actions.get(junc_id, {}))

                    experiences.append({
                        'junction_id': junc_id,
                        'state': obs_tensors[junc_id].squeeze(0).cpu().numpy(),
                        'vehicle_obs': {k: v.cpu().numpy() if torch.is_tensor(v) else v for k, v in vehicle_obs[junc_id].items()},
                        'action': {k: v.item() if torch.is_tensor(v) else v for k, v in actions.get(junc_id, {}).items()},
                        'reward': reward,
                        'value': value.item() if torch.is_tensor(value) else value,
                        'log_prob': log_prob
                    })

                    # ç´¯è®¡å¥–åŠ±
                    if junc_id not in total_rewards:
                        total_rewards[junc_id] = 0.0
                    total_rewards[junc_id] += reward
                except Exception as e:
                    worker_logger.debug(f"å­˜å‚¨è·¯å£ {junc_id} ç»éªŒå¤±è´¥: {e}")

            obs = next_obs
            step_count += 1

            # æ¯1000æ­¥è®°å½•ä¸€æ¬¡è¿›åº¦
            if step_count % 1000 == 0:
                worker_logger.info(f"å·²è¿è¡Œ {step_count} æ­¥")

            if done:
                break

        try:
            env.close()
        except Exception as e:
            worker_logger.warning(f"å…³é—­ç¯å¢ƒæ—¶å‡ºé”™: {e}")

        episode_time = time.time() - episode_start
        worker_logger.info(f"Worker {worker_id} å®Œæˆï¼Œæ”¶é›† {len(experiences)} æ­¥ç»éªŒï¼Œè€—æ—¶ {episode_time:.1f}ç§’")

        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = os.path.join(output_dir, f'worker_{worker_id}.pkl')
        result_data = {
            'worker_id': worker_id,
            'experiences': experiences,
            'total_rewards': total_rewards,
            'steps': len(experiences)
        }

        try:
            with open(output_file, 'wb') as f:
                pickle.dump(result_data, f)

            with open(os.path.join(output_dir, f'worker_{worker_id}.done'), 'w') as f:
                f.write('done')

            worker_logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        except Exception as e:
            worker_logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        worker_logger.error(f"Worker {worker_id} å‘ç”Ÿé”™è¯¯:\n{error_msg}")

        try:
            with open(os.path.join(output_dir, f'worker_{worker_id}.error'), 'w') as f:
                f.write(error_msg)
        except Exception as save_error:
            worker_logger.error(f"ä¿å­˜é”™è¯¯ä¿¡æ¯å¤±è´¥: {save_error}")


def _get_vehicle_features(vehicle_ids, device):
    """è·å–è½¦è¾†ç‰¹å¾"""
    if not vehicle_ids:
        return None

    MAX_VEHICLES = 300  # æœ€å¤§è½¦è¾†æ•°
    features = []
    for veh_id in vehicle_ids[:MAX_VEHICLES]:
        try:
            features.append([
                normalize_speed(traci_wrapper.vehicle.getSpeed(veh_id)),
                traci_wrapper.vehicle.getLanePosition(veh_id) / 500.0,
                traci_wrapper.vehicle.getLaneIndex(veh_id) / 3.0,
                traci_wrapper.vehicle.getWaitingTime(veh_id) / 60.0,
                traci_wrapper.vehicle.getAcceleration(veh_id) / 5.0,
                1.0 if traci_wrapper.vehicle.getTypeID(veh_id) == 'CV' else 0.0,
                traci_wrapper.vehicle.getRouteIndex(veh_id) / 10.0,
                0.0
            ])
        except Exception as e:
            print(f"è·å–è½¦è¾† {veh_id} ç‰¹å¾å¤±è´¥: {e}")
            continue

    if not features:
        return None

    # å¡«å……åˆ°MAX_VEHICLES
    while len(features) < MAX_VEHICLES:
        features.append([0.0] * 8)

    # è¿”å›2Då¼ é‡ [N, 8]ï¼Œè®©æ”¶é›†ä»£ç å¤„ç†batchç»´åº¦
    return torch.tensor(features, dtype=torch.float32, device=device)


def _compute_log_prob(info, actions):
    """è®¡ç®—å¯¹æ•°æ¦‚ç‡"""
    log_prob = 0.0
    for key in ['main', 'ramp', 'diverge']:
        if f'{key}_probs' in info and key in actions:
            probs = info[f'{key}_probs']
            action = actions[key]
            if torch.is_tensor(probs) and torch.is_tensor(action):
                action_idx = int(action.item() * 10)
                action_idx = min(action_idx, probs.size(-1) - 1)
                log_prob += torch.log(probs[0, action_idx] + 1e-8).item()
    return log_prob


def train(args):
    """è®­ç»ƒå‡½æ•°"""
    print_header("å¤šæ™ºèƒ½ä½“è·¯å£æ§åˆ¶ - è®­ç»ƒ")

    # ç¯å¢ƒæ£€æŸ¥
    check_environment()

    # é…ç½®
    net_config = NetworkConfig()
    ppo_config = PPOConfig()

    if args.lr:
        ppo_config.lr = args.lr
    if args.batch_size:
        ppo_config.batch_size = args.batch_size

    num_workers = args.workers or multiprocessing.cpu_count()
    num_envs = min(args.num_envs, num_workers)

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  SUMOé…ç½®: {args.sumo_cfg}")
    print(f"  æ€»æ­¥æ•°: {args.total_timesteps}")
    print(f"  å­¦ä¹ ç‡: {ppo_config.lr}")
    print(f"  æ‰¹å¤§å°: {ppo_config.batch_size}")
    print(f"  è®¾å¤‡: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print(f"  å¹¶è¡Œç¯å¢ƒ: {num_envs}")
    print(f"  å·¥ä½œè¿›ç¨‹: {num_workers}")

    # åˆ›å»ºæ¨¡å‹
    model = create_junction_model(JUNCTION_CONFIGS, net_config)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=ppo_config.lr)

    # ç»éªŒç¼“å†²åŒº
    from junction_trainer import ExperienceBuffer
    buffer = ExperienceBuffer()

    # TensorBoard
    from torch.utils.tensorboard import SummaryWriter
    writer = SummaryWriter(args.log_dir)

    # ä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(os.getcwd(), 'tmp')
    os.makedirs(temp_dir, exist_ok=True)
    print(f"  ä¸´æ—¶ç›®å½•: {temp_dir}")

    # è®¡ç®—æ€»å…±éœ€è¦çš„è¿­ä»£æ¬¡æ•°
    num_iterations = (args.total_timesteps + args.update_frequency * num_workers - 1) // (args.update_frequency * num_workers)

    # è®­ç»ƒå¾ªç¯
    timesteps = 0
    best_ocr = 0.0
    entropy_coef = ppo_config.entropy_coef

    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"é¢„è®¡è¿­ä»£æ¬¡æ•°: {num_iterations}")
    print(f"æ¯æ¬¡è¿­ä»£æ­¥æ•°: ~{args.update_frequency * num_workers}")
    print("=" * 70)

    try:
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(range(num_iterations), desc="è®­ç»ƒè¿›åº¦", unit="iter",
                    ncols=120, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')

        for iteration in pbar:
            start_time = time.time()

            # æ¸…ç©ºä¸´æ—¶ç›®å½•
            for f in os.listdir(temp_dir):
                try:
                    os.remove(os.path.join(temp_dir, f))
                except Exception as e:
                    print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {f} å¤±è´¥: {e}")

            # å¯åŠ¨å·¥ä½œè¿›ç¨‹ï¼ˆæ¯ä¸ªworkerä½¿ç”¨ä¸åŒçš„ç§å­ï¼‰
            processes = []
            use_cuda = torch.cuda.is_available()  # åªè¦æœ‰CUDAå°±ä½¿ç”¨ï¼Œworkerså¯ä»¥å…±äº«GPU

            for worker_id in range(num_workers):
                worker_seed = 42 + worker_id + iteration * 100  # æ¯æ¬¡è¿­ä»£ä¹Ÿä½¿ç”¨ä¸åŒçš„ç§å­
                p = Process(
                    target=worker_process,
                    args=(worker_id, args.sumo_cfg, temp_dir, worker_seed,
                          model.state_dict(),
                          use_cuda)  # ä¼ é€’use_cudaæ ‡å¿—
                )
                p.start()
                processes.append(p)

            # ç­‰å¾…å®Œæˆ
            for p in processes:
                p.join(timeout=600)
                if p.is_alive():
                    p.terminate()

            # è¯»å–ç»“æœï¼ˆä½¿ç”¨tqdmæ˜¾ç¤ºï¼‰
            total_rewards = {}
            total_steps = 0
            worker_stats = []

            for worker_id in tqdm(range(num_workers), desc="  æ”¶é›†æ•°æ®", leave=False, ncols=100):
                result_file = os.path.join(temp_dir, f'worker_{worker_id}.pkl')
                error_file = os.path.join(temp_dir, f'worker_{worker_id}.error')

                if os.path.exists(error_file):
                    with open(error_file, 'r') as f:
                        error_msg = f.read()
                    tqdm.write(f"  âŒ Worker {worker_id} é”™è¯¯: {error_msg[:50]}...")
                    continue

                if os.path.exists(result_file):
                    try:
                        with open(result_file, 'rb') as f:
                            result_data = pickle.load(f)

                        exp_count = len(result_data.get('experiences', []))
                        tqdm.write(f"  ğŸ“¦ Worker {worker_id}: è¯»å– {exp_count} æ¡ç»éªŒ")

                        added_count = 0
                        for exp in result_data['experiences']:
                            try:
                                # ä½¿ç”¨pin_memoryåŠ é€ŸCPUåˆ°GPUä¼ è¾“
                                state_tensor = torch.from_numpy(exp['state']).float().pin_memory().to(device, non_blocking=True)

                                # å¤„ç†vehicle_obs - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ­£ç¡®çš„ç±»å‹
                                vehicle_obs = {}
                                for k, v in exp['vehicle_obs'].items():
                                    if isinstance(v, np.ndarray):
                                        # å¼‚æ­¥ä¼ è¾“åˆ°GPU
                                        vehicle_obs[k] = torch.from_numpy(v).float().pin_memory().to(device, non_blocking=True)
                                    elif v is None:
                                        vehicle_obs[k] = None
                                    else:
                                        vehicle_obs[k] = v

                                # ç¡®ä¿actionä¹Ÿæ˜¯æ­£ç¡®çš„æ ¼å¼
                                action = exp['action']
                                if not isinstance(action, dict):
                                    action = {}

                                buffer.add(
                                    exp['junction_id'], state_tensor, vehicle_obs,
                                    action, exp['reward'], exp['value'], exp['log_prob'], False
                                )
                                added_count += 1
                            except Exception as e:
                                tqdm.write(f"  âš ï¸  æ·»åŠ ç»éªŒå¤±è´¥: {e}")
                                continue

                        tqdm.write(f"  âœ… æˆåŠŸæ·»åŠ  {added_count}/{exp_count} æ¡ç»éªŒåˆ°ç¼“å†²åŒº")

                        # æ”¶é›†ç»Ÿè®¡
                        worker_reward = sum(result_data['total_rewards'].values())
                        worker_steps = result_data['steps']
                        worker_stats.append({
                            'worker_id': worker_id,
                            'steps': worker_steps,
                            'reward': worker_reward
                        })

                        for junc_id, reward in result_data['total_rewards'].items():
                            if junc_id not in total_rewards:
                                total_rewards[junc_id] = 0.0
                            total_rewards[junc_id] += reward

                        total_steps += result_data['steps']

                        # æ‰“å°ç¼“å†²åŒºçŠ¶æ€
                        tqdm.write(f"  ğŸ“Š å½“å‰ç¼“å†²åŒºå¤§å°: {len(buffer)}")

                    except Exception as e:
                        tqdm.write(f"  âš ï¸  Worker {worker_id} è¯»å–å¤±è´¥: {e}\n{tb.format_exc()}")

            timesteps += total_steps
            collect_time = time.time() - start_time

            # ========== ä¿å­˜è®­ç»ƒå‰ç»Ÿè®¡ ==========
            buffer_size_before = len(buffer)
            total_rewards_sum = sum(total_rewards.values()) if total_rewards else 0.0
            mean_reward_before = total_rewards_sum / len(total_rewards) if total_rewards else 0.0

            # æ›´æ–°æ¨¡å‹
            update_start = time.time()

            # ä½¿ç”¨æ ‡å‡†è®­ç»ƒå™¨æ›´æ–°
            trainer = MultiAgentPPOTrainer(model, ppo_config, device)
            trainer.buffer = buffer
            trainer.entropy_coef = entropy_coef
            update_result = trainer.update()
            entropy_coef = trainer.entropy_coef

            update_time = time.time() - update_start

            # è®°å½•
            mean_reward = np.mean(list(total_rewards.values())) if total_rewards else 0.0

            writer.add_scalar('train/reward', mean_reward, timesteps)
            writer.add_scalar('train/loss', update_result['loss'], timesteps)
            writer.add_scalar('train/collect_time', collect_time, timesteps)
            writer.add_scalar('train/update_time', update_time, timesteps)
            writer.add_scalar('train/entropy_coef', entropy_coef, timesteps)

            # ========== æ¨¡å‹æ›´æ–°å®Œæˆæ—¥å¿— ==========
            tqdm.write(f"\n{'='*70}")
            tqdm.write(f"ğŸ”„ æ¨¡å‹æ›´æ–°å®Œæˆ - è¿­ä»£ {iteration + 1}/{num_iterations}")
            tqdm.write(f"{'='*70}")
            tqdm.write(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
            tqdm.write(f"  - æ€»æ­¥æ•°: {timesteps:,} / {args.total_timesteps:,} ({timesteps/args.total_timesteps*100:.1f}%)")
            tqdm.write(f"  - æœ¬æ¬¡æ”¶é›†: {total_steps:,} æ­¥")
            tqdm.write(f"  - è®­ç»ƒå‰ç¼“å†²åŒº: {buffer_size_before:,} æ ·æœ¬")  # ä½¿ç”¨è®­ç»ƒå‰çš„å¤§å°
            tqdm.write(f"  - è®­ç»ƒåç¼“å†²åŒº: {len(buffer):,} æ ·æœ¬ (å·²æ¸…ç©º)")
            tqdm.write(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
            tqdm.write(f"  - æ•°æ®æ”¶é›†: {collect_time:.1f}ç§’")
            tqdm.write(f"  - æ¨¡å‹æ›´æ–°: {update_time:.1f}ç§’")
            tqdm.write(f"  - æ€»è€—æ—¶: {collect_time + update_time:.1f}ç§’")
            tqdm.write(f"\nğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
            tqdm.write(f"  - å¹³å‡å¥–åŠ±: {mean_reward_before:.4f}")  # ä½¿ç”¨è®­ç»ƒå‰è®¡ç®—çš„å¥–åŠ±
            tqdm.write(f"  - æŸå¤±: {update_result['loss']:.4f}")
            tqdm.write(f"  - ç†µç³»æ•°: {entropy_coef:.6f}")
            tqdm.write(f"\nğŸ¢ è·¯å£å¥–åŠ±è¯¦æƒ…:")
            for junc_id, reward in sorted(total_rewards.items()):
                tqdm.write(f"  - {junc_id}: {reward:.4f}")
            tqdm.write(f"{'='*70}\n")

            # æ›´æ–°è¿›åº¦æ¡åç¼€
            pbar.set_postfix({
                'steps': f'{timesteps:,}',
                'reward': f'{mean_reward:.2f}',
                'loss': f'{update_result["loss"]:.4f}',
                'col_t': f'{collect_time:.1f}s',
                'upd_t': f'{update_time:.1f}s'
            })

            # ========== ä¿å­˜æ£€æŸ¥ç‚¹å¹¶å¯åŠ¨å¼‚æ­¥è¯„ä¼° ==========
            # æ¯5æ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (iteration + 1) % 5 == 0:
                # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
                os.makedirs(args.save_dir, exist_ok=True)
                checkpoint_path = os.path.join(args.save_dir, f'checkpoint_iter_{iteration+1:04d}.pt')
                torch.save(model.state_dict(), checkpoint_path)
                tqdm.write(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\n")

                # å¯åŠ¨å¼‚æ­¥è¯„ä¼°
                tqdm.write(f"ğŸš€ å¯åŠ¨å¼‚æ­¥è¯„ä¼°ï¼ˆåå°è¿è¡Œï¼Œä¸é˜»å¡è®­ç»ƒï¼‰...")
                eval_thread = start_async_evaluation(
                    model_path=checkpoint_path,
                    sumo_cfg=args.sumo_cfg,
                    iteration=iteration + 1,
                    eval_dir=os.path.join(args.save_dir, 'evaluations'),
                    device=device
                )
                tqdm.write(f"âœ… è¯„ä¼°è¿›ç¨‹å·²å¯åŠ¨ï¼ˆè¿­ä»£ {iteration + 1}ï¼‰\n")

            # æ¯10æ¬¡è¿­ä»£æ‰“å°è¯¦ç»†ä¿¡æ¯
            if (iteration + 1) % 10 == 0:
                tqdm.write(f"  Workerç»Ÿè®¡:")
                for stat in worker_stats:
                    tqdm.write(f"    Worker {stat['worker_id']}: {stat['steps']:,} æ­¥, å¥–åŠ±: {stat['reward']:.2f}")

        # å…³é—­è¿›åº¦æ¡
        pbar.close()

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for f in os.listdir(temp_dir):
            try:
                os.remove(os.path.join(temp_dir, f))
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {f} å¤±è´¥: {e}")
        writer.close()

    # ä¿å­˜æ¨¡å‹
    os.makedirs(args.save_dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(args.save_dir, 'final_model.pt'))
    print(f"\næ¨¡å‹å·²ä¿å­˜: {args.save_dir}/final_model.pt")


def main():
    parser = argparse.ArgumentParser(description='å¤šæ™ºèƒ½ä½“è·¯å£æ§åˆ¶ - è®­ç»ƒ')

    parser.add_argument('--sumo-cfg', type=str, required=True, help='SUMOé…ç½®æ–‡ä»¶')
    parser.add_argument('--total-timesteps', type=int, default=1000000, help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--batch-size', type=int, default=4096, help='æ‰¹å¤§å°')
    parser.add_argument('--num-envs', type=int, default=4, help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--workers', type=int, help='å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤=CPUæ ¸å¿ƒæ•°ï¼‰')
    parser.add_argument('--update-frequency', type=int, default=2048, help='æ›´æ–°é¢‘ç‡')
    parser.add_argument('--save-dir', type=str, default='checkpoints', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--log-dir', type=str, default='logs', help='æ—¥å¿—ç›®å½•')

    args = parser.parse_args()

    train(args)


if __name__ == '__main__':
    main()
