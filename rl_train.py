"""
å¤šæ™ºèƒ½ä½“è·¯å£äº¤é€šæ§åˆ¶ç³»ç»Ÿ - ç®€åŒ–ç‰ˆ
åªæ”¯æŒCUDAè®­ç»ƒ + æ–‡ä»¶IOå¹¶è¡Œæ•°æ®æ”¶é›†
"""

import os
from vehicle_type_config import normalize_speed, get_vehicle_max_speed
import sys
import argparse
import json
import time
import shutil
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
import pickle
from multiprocessing import Process
import multiprocessing
import subprocess
import threading

# ============== è®¢é˜…æ¨¡å¼ä¼˜åŒ– ==============
# ä½¿ç”¨è®¢é˜…æ¨¡å¼æå‡æ•°æ®æ”¶é›†é€Ÿåº¦ 7-8x
from junction_agent_subscription import JUNCTION_CONFIGS
# ==========================================

from junction_network import create_junction_model, NetworkConfig
from junction_trainer import PPOConfig, MultiAgentPPOTrainer

# å°è¯•å¯¼å…¥libsumo
try:
    import libsumo as traci_wrapper
    USE_LIBSUMO = True
except ImportError:
    import traci as traci_wrapper
    USE_LIBSUMO = False


def print_header(title: str):
    print("=" * 70)
    print(title)
    print("=" * 70)


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("\nç¯å¢ƒæ£€æŸ¥:")

    try:
        import libsumo
        print("  âœ“ libsumo å¯ç”¨ï¼ˆé«˜é€Ÿæ¨¡å¼ï¼‰")
    except ImportError:
        print("  âš  libsumo ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ traci")

    cuda_available = torch.cuda.is_available()
    print(f"  âœ“ CUDA: {cuda_available}")
    if cuda_available:
        print(f"    GPU: {torch.cuda.get_device_name(0)}")
        print(f"    GPUæ•°é‡: {torch.cuda.device_count()}")

    cpu_count = multiprocessing.cpu_count()
    print(f"  âœ“ CPUæ ¸å¿ƒæ•°: {cpu_count}")

    # æ£€æµ‹WSL
    if os.path.exists('/proc/version'):
        with open('/proc/version', 'r') as f:
            if 'microsoft' in f.read().lower():
                print("  âœ“ WSL ç¯å¢ƒ")

    print(f"\næ¨èé…ç½®: --num-envs {min(4, cpu_count)}")


def start_async_evaluation(model_path, sumo_cfg, iteration, eval_dir='evaluations', device='cuda'):
    """
    å¯åŠ¨å¼‚æ­¥è¯„ä¼°è¿›ç¨‹ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        sumo_cfg: SUMOé…ç½®æ–‡ä»¶
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        eval_dir: è¯„ä¼°ç»“æœç›®å½•
        device: è®¾å¤‡
    """
    def run_in_thread():
        try:
            cmd = [
                sys.executable, 'evaluate_model.py',
                '--model-path', model_path,
                '--sumo-cfg', sumo_cfg,
                '--iteration', str(iteration),
                '--eval-dir', eval_dir,
                '--device', device
            ]

            # å¯åŠ¨è¯„ä¼°è¿›ç¨‹ï¼ˆä¸ç­‰å¾…ï¼‰
            subprocess.run(cmd, check=True, capture_output=True, text=True)
        except Exception as e:
            print(f"  âš ï¸  å¼‚æ­¥è¯„ä¼°å¤±è´¥: {e}")

    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
    thread = threading.Thread(target=run_in_thread, daemon=True)
    thread.start()

    return thread


def create_libsumo_environment(sumo_cfg: str, seed: int = 42):
    """åˆ›å»ºlibsumoç¯å¢ƒ"""
    import logging
    import traceback as tb

    # ä¸ºworkeré…ç½®æ—¥å¿—
    worker_logger = logging.getLogger(f'sumo_worker')
    if not worker_logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))
        worker_logger.addHandler(handler)
        worker_logger.setLevel(logging.INFO)

    import junction_agent_subscription  # å¯¼å…¥æ¨¡å—æœ¬èº«ï¼ˆç”¨äºè®¾ç½®traciè¿æ¥ï¼‰
    from junction_agent_subscription import JunctionAgent, SubscriptionManager

    class Environment:
        def __init__(self, sumo_cfg: str, seed: int):
            self.sumo_cfg = sumo_cfg
            self.seed = seed
            self.agents = {}
            self.is_running = False
            self.current_step = 0
            self.logger = worker_logger

            # åˆ›å»ºè®¢é˜…ç®¡ç†å™¨ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
            self.sub_manager = SubscriptionManager()

            try:
                for junc_id in JUNCTION_CONFIGS.keys():
                    self.agents[junc_id] = JunctionAgent(
                        JUNCTION_CONFIGS[junc_id],
                        self.sub_manager
                    )
                self.logger.info(f"Environmentåˆå§‹åŒ–å®Œæˆï¼ˆè®¢é˜…æ¨¡å¼ï¼‰ï¼Œç§å­={seed}")
            except Exception as e:
                self.logger.error(f"Environmentåˆå§‹åŒ–å¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def reset(self):
            """é‡ç½®ç¯å¢ƒå¹¶åº”ç”¨CACCå‚æ•°"""
            try:
                self._start_sumo()
                self.current_step = 0

                for agent in self.agents.values():
                    agent.state_history.clear()

                for _ in range(10):
                    traci_wrapper.simulationStep()
                    self.current_step += 1

                # è®¾ç½®è®¢é˜…ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
                self._setup_subscriptions()

                # åº”ç”¨CACCå‚æ•°ä¼˜åŒ–ï¼ˆä¸æ¨ç†ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼‰
                self._apply_cacc_parameters()

                observations = {junc_id: self.agents[junc_id].observe() for junc_id in self.agents.keys()}
                self.logger.info(f"ç¯å¢ƒé‡ç½®å®Œæˆï¼ˆè®¢é˜…æ¨¡å¼ï¼‰ï¼Œcurrent_step={self.current_step}")
                return observations

            except Exception as e:
                self.logger.error(f"ç¯å¢ƒresetå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _setup_subscriptions(self):
            """è®¾ç½®æ‰€æœ‰è·¯å£çš„è®¢é˜…ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰"""
            try:
                for agent in self.agents.values():
                    agent.setup_subscriptions()
                self.logger.info(f"è®¢é˜…è®¾ç½®å®Œæˆï¼Œè¦†ç›– {len(self.agents)} ä¸ªè·¯å£")
            except Exception as e:
                self.logger.error(f"è®¾ç½®è®¢é˜…å¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def step(self, actions):
            """æ‰§è¡Œä¸€æ­¥"""
            import time
            try:
                step_start = time.time()

                # åº”ç”¨åŠ¨ä½œ
                self._apply_actions(actions)

                # ä»¿çœŸä¸€æ­¥
                traci_wrapper.simulationStep()
                self.current_step += 1

                # è§‚å¯Ÿæ–°çŠ¶æ€ï¼ˆè®¢é˜…æ¨¡å¼ä¼˜åŒ–ï¼‰
                obs_start = time.time()
                observations = {junc_id: self.agents[junc_id].observe() for junc_id in self.agents.keys()}
                obs_time = (time.time() - obs_start) * 1000  # ms

                # è®¡ç®—å¥–åŠ±
                rewards = self._compute_rewards()
                done = self.current_step >= 3600

                # æ€§èƒ½ç›‘æ§ï¼ˆæ¯100æ­¥è®°å½•ä¸€æ¬¡ï¼‰
                if self.current_step % 100 == 0:
                    step_time = (time.time() - step_start) * 1000  # ms
                    self.logger.debug(f"Step {self.current_step}: æ€»è€—æ—¶={step_time:.1f}ms, è§‚å¯Ÿ={obs_time:.1f}ms")

                return observations, rewards, done, {}

            except Exception as e:
                self.logger.error(f"ç¯å¢ƒstepå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _start_sumo(self):
            """å¯åŠ¨SUMO"""
            import sys
            import traci as traci_global  # å¯¼å…¥å…¨å±€traciæ¨¡å—

            try:
                if self.is_running:
                    try:
                        traci_wrapper.close()
                        self.logger.debug("å…³é—­æ—§çš„SUMOè¿æ¥")
                    except Exception as e:
                        self.logger.warning(f"å…³é—­SUMOè¿æ¥æ—¶å‡ºé”™: {e}")

                sumo_binary = "sumo"

                if USE_LIBSUMO:
                    sumo_cmd = [sumo_binary, "-c", self.sumo_cfg, "--no-warnings", "true", "--seed", str(self.seed)]
                    traci_wrapper.start(sumo_cmd)
                else:
                    sumo_cmd = [sumo_binary, "-c", self.sumo_cfg, "--remote-port", "0", "--no-warnings", "true", "--seed", str(self.seed)]
                    traci_wrapper.start(sumo_cmd)

                self.is_running = True
                self.logger.info(f"SUMOå·²å¯åŠ¨ (seed={self.seed})")

                # å…³é”®ä¿®å¤ï¼šè®¾ç½®è®¢é˜…æ¨¡å¼æ¨¡å—çš„traciè¿æ¥
                # 1. è®¾ç½®å…¨å±€traciæ¨¡å—ï¼ˆsys.modulesï¼‰
                sys.modules['traci'] = traci_wrapper
                # 2. ç›´æ¥è®¾ç½®è®¢é˜…æ¨¡å¼æ¨¡å—çš„traciå±æ€§ï¼ˆå› ä¸ºæ¨¡å—çº§åˆ«å¼•ç”¨å·²å›ºå®šï¼‰
                junction_agent_subscription.traci = traci_wrapper
                self.logger.debug("å·²è®¾ç½®traciè¿æ¥ï¼ˆè®¢é˜…æ¨¡å¼å…¼å®¹ï¼‰")

            except Exception as e:
                self.logger.error(f"å¯åŠ¨SUMOå¤±è´¥: {e}\n{tb.format_exc()}")
                raise

        def _apply_cacc_parameters(self):
            """
            åº”ç”¨CACCå‚æ•°ä¼˜åŒ–

            æ ¸å¿ƒç­–ç•¥ï¼š
            - sigma=0: æ¶ˆé™¤éšæœºå‡é€Ÿï¼ˆå®Œç¾é©¾é©¶ï¼‰ï¼Œæé«˜äº¤é€šæµç¨³å®šæ€§
            - tau=1.12: å¾®å¢è·Ÿè½¦æ—¶è·ï¼ˆæŠµæ¶ˆsigma=0å¸¦æ¥çš„å®¹é‡å¢åŠ ï¼Œä¿æŒå®‰å…¨æ€§ï¼‰

            è¿™ä¸ªè®¾ç½®ä¸æ¨ç†ç¯å¢ƒå®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†çš„åŠ¨ä½œç©ºé—´ä¸€è‡´ã€‚
            """
            cacc_applied = set()  # è·Ÿè¸ªå·²è®¾ç½®çš„è½¦è¾†ï¼Œé¿å…é‡å¤è®¾ç½®
            failed_vehicles = []  # è®°å½•å¤±è´¥çš„è½¦è¾†

            try:
                all_vehicles = traci_wrapper.vehicle.getIDList()
                self.logger.debug(f"å¼€å§‹åº”ç”¨CACCå‚æ•°ï¼Œè½¦è¾†æ€»æ•°={len(all_vehicles)}")

                for veh_id in all_vehicles:
                    if veh_id in cacc_applied:
                        continue

                    try:
                        # åªå¯¹CVï¼ˆConnected Vehicleï¼‰ç±»å‹åº”ç”¨CACCå‚æ•°
                        veh_type = traci_wrapper.vehicle.getTypeID(veh_id)
                        if veh_type == 'CV':
                            # è®¾ç½®imperfectionï¼ˆsigmaï¼‰ä¸º0ï¼Œæ¶ˆé™¤éšæœºå‡é€Ÿ
                            traci_wrapper.vehicle.setImperfection(veh_id, 0.0)

                            # è®¾ç½®tauï¼ˆè·Ÿè½¦æ—¶è·ï¼‰ä¸º1.12ç§’ï¼Œç•¥å¾®å¢å¤§ä»¥ä¿æŒå®‰å…¨è·ç¦»
                            traci_wrapper.vehicle.setTau(veh_id, 1.12)

                            cacc_applied.add(veh_id)
                    except Exception as e:
                        # è½¦è¾†å¯èƒ½åœ¨è®¾ç½®è¿‡ç¨‹ä¸­ç¦»å¼€è·¯ç½‘ï¼Œè®°å½•ä½†ä¸ä¸­æ–­
                        failed_vehicles.append((veh_id, str(e)))

                self.logger.info(f"CACCå‚æ•°åº”ç”¨å®Œæˆ: æˆåŠŸ={len(cacc_applied)}è¾†, å¤±è´¥={len(failed_vehicles)}è¾†")
                if failed_vehicles and len(failed_vehicles) <= 5:
                    for veh_id, err in failed_vehicles[:5]:
                        self.logger.debug(f"  è½¦è¾† {veh_id} è®¾ç½®å¤±è´¥: {err}")

            except Exception as e:
                self.logger.error(f"åº”ç”¨CACCå‚æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}\n{tb.format_exc()}")

        def _apply_actions(self, actions):
            """åº”ç”¨åŠ¨ä½œåˆ°è½¦è¾†"""
            failed_count = 0

            for junc_id, action_dict in actions.items():
                for veh_id, action in action_dict.items():
                    try:
                        speed_limit = 13.89
                        target_speed = speed_limit * (0.3 + 0.9 * action)
                        traci_wrapper.vehicle.setSpeed(veh_id, target_speed)
                    except Exception as e:
                        failed_count += 1
                        if failed_count <= 3:  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                            self.logger.debug(f"è®¾ç½®è½¦è¾† {veh_id} é€Ÿåº¦å¤±è´¥: {e}")

            if failed_count > 3:
                self.logger.debug(f"æ€»è®¡ {failed_count} ä¸ªè½¦è¾†é€Ÿåº¦è®¾ç½®å¤±è´¥")

        def _compute_rewards(self):
            """è®¡ç®—å¥–åŠ±"""
            rewards = {}
            for junc_id, agent in self.agents.items():
                try:
                    state = agent.current_state
                    if state is None:
                        rewards[junc_id] = 0.0
                        continue

                    throughput = -state.main_queue_length * 0.1 - state.ramp_queue_length * 0.2
                    waiting = -state.ramp_waiting_time * 0.05
                    conflict = -state.conflict_risk * 0.5
                    gap = state.gap_acceptance * 0.2 if state.ramp_vehicles else 0
                    speed_stability = -abs(state.main_speed - state.ramp_speed) * 0.02

                    rewards[junc_id] = throughput + waiting + conflict + gap + speed_stability

                except Exception as e:
                    self.logger.warning(f"è®¡ç®—è·¯å£ {junc_id} å¥–åŠ±å¤±è´¥: {e}")
                    rewards[junc_id] = 0.0

            return rewards

        def close(self):
            """å…³é—­ç¯å¢ƒ"""
            if self.is_running:
                try:
                    traci_wrapper.close()
                    self.logger.info("SUMOè¿æ¥å·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­SUMOè¿æ¥æ—¶å‡ºé”™: {e}")
                self.is_running = False

    return Environment(sumo_cfg, seed)


def worker_process(worker_id, sumo_cfg, output_dir, seed, model_state, use_cuda):
    """å·¥ä½œè¿›ç¨‹ - æ–‡ä»¶IOç‰ˆæœ¬"""
    import traceback
    import logging

    # é…ç½®workeræ—¥å¿—
    worker_logger = logging.getLogger(f'worker_{worker_id}')
    if not worker_logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter(f'[Worker-{worker_id}] [%(levelname)s] %(message)s'))
        worker_logger.addHandler(handler)
        worker_logger.setLevel(logging.INFO)

    try:
        import time
        worker_logger.info(f"Worker {worker_id} å¯åŠ¨ï¼Œseed={seed}")
        worker_start = time.time()

        np.random.seed(seed + worker_id)
        torch.manual_seed(seed + worker_id)

        device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'
        worker_logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # åˆ›å»ºç¯å¢ƒ
        env = create_libsumo_environment(sumo_cfg, seed)
        worker_logger.info("ç¯å¢ƒåˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºæ¨¡å‹
        model = create_junction_model(JUNCTION_CONFIGS)
        model.load_state_dict(model_state)
        model.to(device)
        model.eval()
        worker_logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")

        # æ”¶é›†ç»éªŒ - è¿è¡Œå®Œæ•´çš„3600æ­¥episode
        episode_start = time.time()
        obs = env.reset()
        experiences = []
        total_rewards = {}
        step_count = 0

        # è¿è¡Œå®Œæ•´çš„episodeï¼Œç›´åˆ°ç¯å¢ƒdone
        while True:
            # å‡†å¤‡è§‚å¯Ÿ
            obs_tensors = {}
            vehicle_obs = {}

            for junc_id, agent in env.agents.items():
                try:
                    state_vec = agent.get_state_vector()
                    obs_tensors[junc_id] = torch.tensor(state_vec, dtype=torch.float32, device=device).unsqueeze(0)

                    controlled = agent.get_controlled_vehicles()
                    vehicle_obs[junc_id] = {
                        'main': _get_vehicle_features(controlled['main'], device) if controlled['main'] else None,
                        'ramp': _get_vehicle_features(controlled['ramp'], device) if controlled['ramp'] else None,
                        'diverge': _get_vehicle_features(controlled['diverge'], device) if controlled['diverge'] else None
                    }
                except Exception as e:
                    worker_logger.debug(f"è·¯å£ {junc_id} è§‚å¯Ÿå¤±è´¥: {e}")

            if not obs_tensors:
                worker_logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è§‚å¯Ÿï¼Œè·³è¿‡æ­¤æ­¥")
                break

            # è·å–åŠ¨ä½œ
            try:
                with torch.no_grad():
                    actions, values, info = model(obs_tensors, vehicle_obs, deterministic=False)
            except Exception as e:
                worker_logger.error(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                break

            # è½¬æ¢åŠ¨ä½œ
            action_dict = {}
            for junc_id, action in actions.items():
                action_dict[junc_id] = {}
                try:
                    controlled = env.agents[junc_id].get_controlled_vehicles()

                    if controlled['main'] and 'main' in action:
                        for veh_id in controlled['main'][:1]:
                            action_dict[junc_id][veh_id] = action['main'].item()

                    if controlled['ramp'] and 'ramp' in action:
                        for veh_id in controlled['ramp'][:1]:
                            action_dict[junc_id][veh_id] = action['ramp'].item()
                except Exception as e:
                    worker_logger.debug(f"è·¯å£ {junc_id} åŠ¨ä½œè½¬æ¢å¤±è´¥: {e}")

            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                next_obs, rewards, done, info = env.step(action_dict)
            except Exception as e:
                worker_logger.error(f"ç¯å¢ƒstepå¤±è´¥: {e}\n{traceback.format_exc()}")
                break

            # å­˜å‚¨ç»éªŒï¼ˆç°åœ¨å¯ä»¥è·å–rewardäº†ï¼‰
            for junc_id in env.agents.keys():
                try:
                    reward = rewards.get(junc_id, 0.0)
                    value = values.get(junc_id, torch.tensor(0.0))
                    log_prob = _compute_log_prob(info.get(junc_id, {}), actions.get(junc_id, {}))

                    experiences.append({
                        'junction_id': junc_id,
                        'state': obs_tensors[junc_id].squeeze(0).cpu().numpy(),
                        'vehicle_obs': {k: v.cpu().numpy() if torch.is_tensor(v) else v for k, v in vehicle_obs[junc_id].items()},
                        'action': {k: v.item() if torch.is_tensor(v) else v for k, v in actions.get(junc_id, {}).items()},
                        'reward': reward,
                        'value': value.item() if torch.is_tensor(value) else value,
                        'log_prob': log_prob
                    })

                    # ç´¯è®¡å¥–åŠ±
                    if junc_id not in total_rewards:
                        total_rewards[junc_id] = 0.0
                    total_rewards[junc_id] += reward
                except Exception as e:
                    worker_logger.debug(f"å­˜å‚¨è·¯å£ {junc_id} ç»éªŒå¤±è´¥: {e}")

            obs = next_obs
            step_count += 1

            # æ¯1000æ­¥è®°å½•ä¸€æ¬¡è¿›åº¦
            if step_count % 1000 == 0:
                worker_logger.info(f"å·²è¿è¡Œ {step_count} æ­¥")

            if done:
                break

        try:
            env.close()
        except Exception as e:
            worker_logger.warning(f"å…³é—­ç¯å¢ƒæ—¶å‡ºé”™: {e}")

        episode_time = time.time() - episode_start
        worker_logger.info(f"Worker {worker_id} å®Œæˆï¼Œæ”¶é›† {len(experiences)} æ­¥ç»éªŒï¼Œè€—æ—¶ {episode_time:.1f}ç§’")

        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = os.path.join(output_dir, f'worker_{worker_id}.pkl')
        result_data = {
            'worker_id': worker_id,
            'experiences': experiences,
            'total_rewards': total_rewards,
            'steps': len(experiences)
        }

        try:
            with open(output_file, 'wb') as f:
                pickle.dump(result_data, f)

            with open(os.path.join(output_dir, f'worker_{worker_id}.done'), 'w') as f:
                f.write('done')

            worker_logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        except Exception as e:
            worker_logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        worker_logger.error(f"Worker {worker_id} å‘ç”Ÿé”™è¯¯:\n{error_msg}")

        try:
            with open(os.path.join(output_dir, f'worker_{worker_id}.error'), 'w') as f:
                f.write(error_msg)
        except Exception as save_error:
            worker_logger.error(f"ä¿å­˜é”™è¯¯ä¿¡æ¯å¤±è´¥: {save_error}")


def _get_vehicle_features(vehicle_ids, device):
    """è·å–è½¦è¾†ç‰¹å¾"""
    if not vehicle_ids:
        return None

    features = []
    for veh_id in vehicle_ids[:10]:
        try:
            features.append([
                normalize_speed(traci_wrapper.vehicle.getSpeed(veh_id)),
                traci_wrapper.vehicle.getLanePosition(veh_id) / 500.0,
                traci_wrapper.vehicle.getLaneIndex(veh_id) / 3.0,
                traci_wrapper.vehicle.getWaitingTime(veh_id) / 60.0,
                traci_wrapper.vehicle.getAcceleration(veh_id) / 5.0,
                1.0 if traci_wrapper.vehicle.getTypeID(veh_id) == 'CV' else 0.0,
                traci_wrapper.vehicle.getRouteIndex(veh_id) / 10.0,
                0.0
            ])
        except:
            continue

    if not features:
        return None

    return torch.tensor(features, dtype=torch.float32, device=device).unsqueeze(0)


def _compute_log_prob(info, actions):
    """è®¡ç®—å¯¹æ•°æ¦‚ç‡"""
    log_prob = 0.0
    for key in ['main', 'ramp', 'diverge']:
        if f'{key}_probs' in info and key in actions:
            probs = info[f'{key}_probs']
            action = actions[key]
            if torch.is_tensor(probs) and torch.is_tensor(action):
                action_idx = int(action.item() * 10)
                action_idx = min(action_idx, probs.size(-1) - 1)
                log_prob += torch.log(probs[0, action_idx] + 1e-8).item()
    return log_prob


def train(args):
    """è®­ç»ƒå‡½æ•°"""
    print_header("å¤šæ™ºèƒ½ä½“è·¯å£æ§åˆ¶ - è®­ç»ƒ")

    # ç¯å¢ƒæ£€æŸ¥
    check_environment()

    # é…ç½®
    net_config = NetworkConfig()
    ppo_config = PPOConfig()

    if args.lr:
        ppo_config.lr = args.lr
    if args.batch_size:
        ppo_config.batch_size = args.batch_size

    num_workers = args.workers or multiprocessing.cpu_count()
    num_envs = min(args.num_envs, num_workers)

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  SUMOé…ç½®: {args.sumo_cfg}")
    print(f"  æ€»æ­¥æ•°: {args.total_timesteps}")
    print(f"  å­¦ä¹ ç‡: {ppo_config.lr}")
    print(f"  æ‰¹å¤§å°: {ppo_config.batch_size}")
    print(f"  è®¾å¤‡: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print(f"  å¹¶è¡Œç¯å¢ƒ: {num_envs}")
    print(f"  å·¥ä½œè¿›ç¨‹: {num_workers}")

    # åˆ›å»ºæ¨¡å‹
    model = create_junction_model(JUNCTION_CONFIGS, net_config)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=ppo_config.lr)

    # ç»éªŒç¼“å†²åŒº
    from junction_trainer import ExperienceBuffer
    buffer = ExperienceBuffer()

    # TensorBoard
    from torch.utils.tensorboard import SummaryWriter
    writer = SummaryWriter(args.log_dir)

    # ä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(os.getcwd(), 'tmp')
    os.makedirs(temp_dir, exist_ok=True)
    print(f"  ä¸´æ—¶ç›®å½•: {temp_dir}")

    # è®¡ç®—æ€»å…±éœ€è¦çš„è¿­ä»£æ¬¡æ•°
    num_iterations = (args.total_timesteps + args.update_frequency * num_workers - 1) // (args.update_frequency * num_workers)

    # è®­ç»ƒå¾ªç¯
    timesteps = 0
    best_ocr = 0.0
    entropy_coef = ppo_config.entropy_coef

    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"é¢„è®¡è¿­ä»£æ¬¡æ•°: {num_iterations}")
    print(f"æ¯æ¬¡è¿­ä»£æ­¥æ•°: ~{args.update_frequency * num_workers}")
    print("=" * 70)

    try:
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(range(num_iterations), desc="è®­ç»ƒè¿›åº¦", unit="iter",
                    ncols=120, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')

        for iteration in pbar:
            start_time = time.time()

            # æ¸…ç©ºä¸´æ—¶ç›®å½•
            for f in os.listdir(temp_dir):
                try:
                    os.remove(os.path.join(temp_dir, f))
                except:
                    pass

            # å¯åŠ¨å·¥ä½œè¿›ç¨‹ï¼ˆæ¯ä¸ªworkerä½¿ç”¨ä¸åŒçš„ç§å­ï¼‰
            processes = []
            use_cuda = torch.cuda.is_available()  # åªè¦æœ‰CUDAå°±ä½¿ç”¨ï¼Œworkerså¯ä»¥å…±äº«GPU

            for worker_id in range(num_workers):
                worker_seed = 42 + worker_id + iteration * 100  # æ¯æ¬¡è¿­ä»£ä¹Ÿä½¿ç”¨ä¸åŒçš„ç§å­
                p = Process(
                    target=worker_process,
                    args=(worker_id, args.sumo_cfg, temp_dir, worker_seed,
                          model.state_dict(),
                          use_cuda)  # ä¼ é€’use_cudaæ ‡å¿—
                )
                p.start()
                processes.append(p)

            # ç­‰å¾…å®Œæˆ
            for p in processes:
                p.join(timeout=600)
                if p.is_alive():
                    p.terminate()

            # è¯»å–ç»“æœï¼ˆä½¿ç”¨tqdmæ˜¾ç¤ºï¼‰
            total_rewards = {}
            total_steps = 0
            worker_stats = []

            for worker_id in tqdm(range(num_workers), desc="  æ”¶é›†æ•°æ®", leave=False, ncols=100):
                result_file = os.path.join(temp_dir, f'worker_{worker_id}.pkl')
                error_file = os.path.join(temp_dir, f'worker_{worker_id}.error')

                if os.path.exists(error_file):
                    with open(error_file, 'r') as f:
                        error_msg = f.read()
                    tqdm.write(f"  âŒ Worker {worker_id} é”™è¯¯: {error_msg[:50]}...")
                    continue

                if os.path.exists(result_file):
                    try:
                        with open(result_file, 'rb') as f:
                            result_data = pickle.load(f)

                        for exp in result_data['experiences']:
                            # ä½¿ç”¨pin_memoryåŠ é€ŸCPUåˆ°GPUä¼ è¾“
                            state_tensor = torch.from_numpy(exp['state']).float().pin_memory().to(device, non_blocking=True)
                            vehicle_obs = {}
                            for k, v in exp['vehicle_obs'].items():
                                if isinstance(v, np.ndarray):
                                    # å¼‚æ­¥ä¼ è¾“åˆ°GPU
                                    vehicle_obs[k] = torch.from_numpy(v).float().pin_memory().to(device, non_blocking=True)
                                else:
                                    vehicle_obs[k] = v

                            buffer.add(
                                exp['junction_id'], state_tensor, vehicle_obs,
                                exp['action'], exp['reward'], exp['value'], exp['log_prob'], False
                            )

                        # æ”¶é›†ç»Ÿè®¡
                        worker_reward = sum(result_data['total_rewards'].values())
                        worker_steps = result_data['steps']
                        worker_stats.append({
                            'worker_id': worker_id,
                            'steps': worker_steps,
                            'reward': worker_reward
                        })

                        for junc_id, reward in result_data['total_rewards'].items():
                            if junc_id not in total_rewards:
                                total_rewards[junc_id] = 0.0
                            total_rewards[junc_id] += reward

                        total_steps += result_data['steps']

                    except Exception as e:
                        tqdm.write(f"  âš ï¸  Worker {worker_id} è¯»å–å¤±è´¥: {e}")

            timesteps += total_steps
            collect_time = time.time() - start_time

            # æ›´æ–°æ¨¡å‹
            update_start = time.time()

            # ä½¿ç”¨æ ‡å‡†è®­ç»ƒå™¨æ›´æ–°
            trainer = MultiAgentPPOTrainer(model, ppo_config, device)
            trainer.buffer = buffer
            trainer.entropy_coef = entropy_coef
            update_result = trainer.update()
            entropy_coef = trainer.entropy_coef

            update_time = time.time() - update_start

            # è®°å½•
            mean_reward = np.mean(list(total_rewards.values())) if total_rewards else 0.0

            writer.add_scalar('train/reward', mean_reward, timesteps)
            writer.add_scalar('train/loss', update_result['loss'], timesteps)
            writer.add_scalar('train/collect_time', collect_time, timesteps)
            writer.add_scalar('train/update_time', update_time, timesteps)
            writer.add_scalar('train/entropy_coef', entropy_coef, timesteps)

            # ========== æ¨¡å‹æ›´æ–°å®Œæˆæ—¥å¿— ==========
            tqdm.write(f"\n{'='*70}")
            tqdm.write(f"ğŸ”„ æ¨¡å‹æ›´æ–°å®Œæˆ - è¿­ä»£ {iteration + 1}/{num_iterations}")
            tqdm.write(f"{'='*70}")
            tqdm.write(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
            tqdm.write(f"  - æ€»æ­¥æ•°: {timesteps:,} / {args.total_timesteps:,} ({timesteps/args.total_timesteps*100:.1f}%)")
            tqdm.write(f"  - æœ¬æ¬¡æ”¶é›†: {total_steps:,} æ­¥")
            tqdm.write(f"  - ç¼“å†²åŒºå¤§å°: {len(buffer):,} æ ·æœ¬")
            tqdm.write(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
            tqdm.write(f"  - æ•°æ®æ”¶é›†: {collect_time:.1f}ç§’")
            tqdm.write(f"  - æ¨¡å‹æ›´æ–°: {update_time:.1f}ç§’")
            tqdm.write(f"  - æ€»è€—æ—¶: {collect_time + update_time:.1f}ç§’")
            tqdm.write(f"\nğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
            tqdm.write(f"  - å¹³å‡å¥–åŠ±: {mean_reward:.4f}")
            tqdm.write(f"  - æŸå¤±: {update_result['loss']:.4f}")
            tqdm.write(f"  - ç†µç³»æ•°: {entropy_coef:.6f}")
            tqdm.write(f"\nğŸ¢ è·¯å£å¥–åŠ±è¯¦æƒ…:")
            for junc_id, reward in sorted(total_rewards.items()):
                tqdm.write(f"  - {junc_id}: {reward:.4f}")
            tqdm.write(f"{'='*70}\n")

            # æ›´æ–°è¿›åº¦æ¡åç¼€
            pbar.set_postfix({
                'steps': f'{timesteps:,}',
                'reward': f'{mean_reward:.2f}',
                'loss': f'{update_result["loss"]:.4f}',
                'col_t': f'{collect_time:.1f}s',
                'upd_t': f'{update_time:.1f}s'
            })

            # ========== ä¿å­˜æ£€æŸ¥ç‚¹å¹¶å¯åŠ¨å¼‚æ­¥è¯„ä¼° ==========
            # æ¯5æ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (iteration + 1) % 5 == 0:
                checkpoint_path = os.path.join(args.save_dir, f'checkpoint_iter_{iteration+1:04d}.pt')
                torch.save(model.state_dict(), checkpoint_path)
                tqdm.write(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\n")

                # å¯åŠ¨å¼‚æ­¥è¯„ä¼°
                tqdm.write(f"ğŸš€ å¯åŠ¨å¼‚æ­¥è¯„ä¼°ï¼ˆåå°è¿è¡Œï¼Œä¸é˜»å¡è®­ç»ƒï¼‰...")
                eval_thread = start_async_evaluation(
                    model_path=checkpoint_path,
                    sumo_cfg=args.sumo_cfg,
                    iteration=iteration + 1,
                    eval_dir=os.path.join(args.save_dir, 'evaluations'),
                    device=device
                )
                tqdm.write(f"âœ… è¯„ä¼°è¿›ç¨‹å·²å¯åŠ¨ï¼ˆè¿­ä»£ {iteration + 1}ï¼‰\n")

            # æ¯10æ¬¡è¿­ä»£æ‰“å°è¯¦ç»†ä¿¡æ¯
            if (iteration + 1) % 10 == 0:
                tqdm.write(f"  Workerç»Ÿè®¡:")
                for stat in worker_stats:
                    tqdm.write(f"    Worker {stat['worker_id']}: {stat['steps']:,} æ­¥, å¥–åŠ±: {stat['reward']:.2f}")

        # å…³é—­è¿›åº¦æ¡
        pbar.close()

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for f in os.listdir(temp_dir):
            try:
                os.remove(os.path.join(temp_dir, f))
            except:
                pass
        writer.close()

    # ä¿å­˜æ¨¡å‹
    os.makedirs(args.save_dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(args.save_dir, 'final_model.pt'))
    print(f"\næ¨¡å‹å·²ä¿å­˜: {args.save_dir}/final_model.pt")


def main():
    parser = argparse.ArgumentParser(description='å¤šæ™ºèƒ½ä½“è·¯å£æ§åˆ¶ - è®­ç»ƒ')

    parser.add_argument('--sumo-cfg', type=str, required=True, help='SUMOé…ç½®æ–‡ä»¶')
    parser.add_argument('--total-timesteps', type=int, default=1000000, help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹å¤§å°')
    parser.add_argument('--num-envs', type=int, default=4, help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--workers', type=int, help='å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤=CPUæ ¸å¿ƒæ•°ï¼‰')
    parser.add_argument('--update-frequency', type=int, default=2048, help='æ›´æ–°é¢‘ç‡')
    parser.add_argument('--save-dir', type=str, default='checkpoints', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--log-dir', type=str, default='logs', help='æ—¥å¿—ç›®å½•')

    args = parser.parse_args()

    train(args)


if __name__ == '__main__':
    main()
